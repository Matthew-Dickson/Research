{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U44wvsRQFg5K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "3977ce77-60e0-4bd0-a225-3512fc1df5b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA_hLy8hUMYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "adfa504e-5be0-4bd4-9f4c-449aa2415fa9"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdfODzioq5-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn.preprocessing as pp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.datasets as sk\n",
        "import csv\n",
        "import math\n",
        "from decimal import Decimal\n",
        "import random\n",
        "from scipy.stats import mannwhitneyu\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.datasets import fetch_openml\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdeKqLwq7oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def cross_entropy(logits, y):\n",
        "    ce_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\n",
        "    return ce_op\n",
        "\n",
        "@tf.function\n",
        "def mse(pred, y):\n",
        "    mse_op = tf.reduce_mean(tf.square(pred - y))\n",
        "    return mse_op\n",
        "\n",
        "@tf.function\n",
        "def loss_function(labels,predictions,CEscalar,MSEscalar):\n",
        "\n",
        "  labels = tf.dtypes.cast(labels,tf.float64)\n",
        " \n",
        "  CE_object = cross_entropy(predictions,labels)\n",
        "\n",
        "  MSE_object = mse(predictions,labels)\n",
        " \n",
        " \n",
        "  loss_object = (CEscalar*CE_object)+(MSEscalar*MSE_object)\n",
        "\n",
        "  return loss_object\n",
        "\n",
        "@tf.function\n",
        "def model_test(features, labels,model,CEscalar,MSEscalar):\n",
        "    predictions = model(features,training=False)\n",
        "    t_loss = loss_function(labels, predictions, CEscalar, MSEscalar)\n",
        "    test_loss(t_loss)\n",
        "    test_acc(labels, predictions)\n",
        "    \n",
        "\n",
        "\n",
        "@tf.function\n",
        "def model_train(features, labels,model,CEscalar,MSEscalar,optimizer):\n",
        "    # Define the GradientTape context\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get the probabilities\n",
        "        predictions = model(features, training=True)\n",
        "        # Calculate the loss\n",
        "        loss = loss_function(labels, predictions, CEscalar, MSEscalar)   \n",
        "    # Get the gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "    # Update the loss and accuracy\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, predictions)\n",
        "    return gradients \n",
        "       \n",
        "\n",
        "      \n",
        "def PrintFunc(template):\n",
        "    return template.format(epoch+1,loss, acc,tloss,tacc)\n",
        "\n",
        "def shuffle(features,labels):\n",
        "    #zip data\n",
        "    data = list(zip(features,labels))\n",
        "\n",
        "    #shuffle data\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    #unzip data\n",
        "    features,labels = zip(*data)\n",
        "    features = np.asarray(features)\n",
        "    labels = np.asarray(labels)\n",
        "    return features,labels  \n",
        "\n",
        "\n",
        "\n",
        "def MeanAcrossEpoch(arr,num_epoch):\n",
        "    \n",
        "    tot = []\n",
        "   \n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    for i in arr:\n",
        "        val = 0\n",
        "        val = np.mean(i,dtype=np.float64)\n",
        "        tot.append(val)\n",
        "    return tot      \n",
        "\n",
        "\n",
        "def StdAcrossEpoch(arr,num_epoch):\n",
        "    \n",
        "    tot = []\n",
        "   \n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    for i in arr:\n",
        "        val = 0\n",
        "        val = np.std(i,dtype=np.float64,ddof=1)\n",
        "        tot.append(val)\n",
        "    return tot    \n",
        "\n",
        "\n",
        "def createList(r1, r2): \n",
        "  \n",
        "    # Testing if range r1 and r2  \n",
        "    # are equal \n",
        "    if (r1 == r2): \n",
        "        return r1 \n",
        "  \n",
        "    else: \n",
        "  \n",
        "        # Create empty list \n",
        "        res = [] \n",
        "  \n",
        "        # loop to append successors to  \n",
        "        # list until r2 is reached. \n",
        "        while(r1 < r2+1 ): \n",
        "              \n",
        "            res.append(r1) \n",
        "            r1 += 1\n",
        "        return res \n",
        "          \n",
        "\n",
        "def SameEpoch(arr,num_epoch):\n",
        "    new = []\n",
        "    for i in range(num_epoch):\n",
        "        new.append(arr[i::num_epoch])   \n",
        "    return new\n",
        "\n",
        "#Get the mean for each epoch across the different folds\n",
        "def MeanAcrossFolds(arr,num_epoch,num_folds):\n",
        "    tot = []\n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    \n",
        "    for i in range(num_epoch):\n",
        "        val = 0\n",
        "        val = np.sum(arr[i],dtype=np.float64)/num_folds\n",
        "        tot.append(val)\n",
        "    return tot       \n",
        "\n",
        "\n",
        "\n",
        "#Whitney U test\n",
        "def whitney(data,names,alpha=0.05,alt=None,savetoFile = False,file='default'):\n",
        "\n",
        "  if savetoFile == False:\n",
        "\n",
        "    for x in range(len(data)-1):\n",
        "      print('\\n')\n",
        "      for y in range(x+1,len(data)):\n",
        "\n",
        "        print(names[x] + ' Mean is less than ' + names[y])\n",
        "        stat, p = mannwhitneyu(data[x], data[y] , alternative= alt)\n",
        "        print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "        if p > alpha:\n",
        "\t        print('Same distribution (fail to reject H0)')\n",
        "        else:\n",
        "\t        print('Different distribution (reject H0)')\n",
        "  else:\n",
        "      file = open(\"C:/Users/AutoMAttic/Desktop/honours/COS700/Research/glass\"+file,'a')\n",
        "      for x in range(len(data)-1):\n",
        "        file.write('\\n')\n",
        "        file.write('\\n')\n",
        "        for y in range(x+1,len(data)):\n",
        "          file.write(names[x] + ' Checking distibution ' + names[y])\n",
        "          file.write('\\n')\n",
        "          stat, p = mannwhitneyu(data[x], data[y] , alternative= alt)\n",
        "          file.write('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "          file.write('\\n')\n",
        "          if p > alpha:\n",
        "\t          file.write('Same distribution (fail to reject H0)')\n",
        "          else:\n",
        "\t          file.write('Different distribution (reject H0)')     \n",
        "      file.close()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzoj2aaNsIja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Setting Environment\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "\n",
        "\n",
        "# Hyper Parameter\n",
        "LEARNING_RATE = 0.01\n",
        "CE_SCALAR = 1\n",
        "MSE_SCALAR = 0\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "NUM_FOLDS = 10\n",
        "RUNS = 30\n",
        "TOTALEPOCHS = NUM_FOLDS*NUM_EPOCHS*RUNS\n",
        "\n",
        "#Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "train_acc = tf.keras.metrics.CategoricalAccuracy(name=\"train_acc\")\n",
        "test_acc = tf.keras.metrics.CategoricalAccuracy(name=\"test_acc\")\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Load features and labels from iris\n",
        "features, labels = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "features = features.astype('float64')\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(features)\n",
        "features = scaler.transform(features)\n",
        "labels = pp.label_binarize(labels, classes=np.unique(labels))\n",
        "\n",
        "\n",
        "# shuffle data\n",
        "features,labels=shuffle(features,labels)\n",
        "#for stats test\n",
        "StatsforHybrids = []\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwP0M3DjsMeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#static hybrid functions\n",
        "while(CE_SCALAR >= 0):\n",
        "\n",
        "    #Setting seed values for consistent results when different hybrids are used\n",
        "    seed = 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    #Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Declaring arrays for holding mean values across folds\n",
        "    MeantestAccSamples =[]\n",
        "    MeantrainAccSamples = []\n",
        "    MeantestLossSamples =[]\n",
        "    MeantrainLossSamples = []\n",
        "\n",
        "    #Declaring arrays for holding mean values across runs\n",
        "    atr_accuracy_results = []\n",
        "    ate_accuracy_results = []\n",
        "    atr_loss_results = []\n",
        "    ate_loss_results = []\n",
        "\n",
        "    #Defining arrays to be used to flatten values into a one dimensional array \n",
        "    testAccflatten = []\n",
        "    trainAccflatten = []\n",
        "    testLossflatten = []\n",
        "    trainLossflatten = []\n",
        "\n",
        "    #arrays used to capture standard deviation \n",
        "    atrstdacc = []\n",
        "    atestdacc = []\n",
        "    atrstdloss = []\n",
        "    atestdloss = []\n",
        "\n",
        "\n",
        "    for run in tf.range(RUNS):\n",
        "\n",
        "     \n",
        "       \n",
        "\n",
        "       #Constructing the model \n",
        "       model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "                              tf.keras.layers.Dense(10,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(10, activation='softmax'),\n",
        "       ])\n",
        "\n",
        "\n",
        "       #Save weights \n",
        "       Wsave = model.get_weights()\n",
        "       print(\"run {:d}\".format(run+1))\n",
        "\n",
        "       \n",
        "       fold=1 \n",
        "\n",
        "       kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "       # For data in number of folds\n",
        "       for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "           X_train, X_test = features[train], features[test]\n",
        "           y_train, y_test= labels[train], labels[test]    \n",
        "           \n",
        "           #set saved weights\n",
        "           model.set_weights(Wsave) \n",
        "           \n",
        "           print(\"Fold {:d}\".format(fold))\n",
        "           fold = fold + 1 \n",
        "           \n",
        "           #train for a number of epochs\n",
        "           for epoch in tf.range(NUM_EPOCHS):\n",
        "               \n",
        "               #Shuffle dataset for each epoch\n",
        "               tr = list(zip(X_train, y_train))\n",
        "               te = list(zip(X_test, y_test))\n",
        "               np.random.shuffle(tr)\n",
        "               np.random.shuffle(te)\n",
        "               X_train,y_train = zip(*tr) \n",
        "               X_test,y_test = zip(*te) \n",
        "\n",
        "\n",
        "               #Make numpy arrays to feed into model\n",
        "               X_train=np.asarray(X_train, dtype=np.float64)\n",
        "               y_train=np.asarray(y_train, dtype=np.float64)\n",
        "               X_test=np.asarray(X_test, dtype=np.float64)\n",
        "               y_test=np.asarray(y_test, dtype=np.float64)\n",
        "               \n",
        "               \n",
        "               # Train on batch\n",
        "               for i in range(0,len(X_train) // BATCH_SIZE):\n",
        "                   X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                   y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                   gradients=model_train(X, y,model,CE_SCALAR,MSE_SCALAR,optimizer)\n",
        "                   optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "               # Evaluate for each epoch    \n",
        "               model_test(X_test, y_test,model,CE_SCALAR,MSE_SCALAR)   \n",
        "\n",
        "               # Grab the results\n",
        "               (loss, acc) = train_loss.result(), train_acc.result()\n",
        "               (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "               #Appending values for each epoch \n",
        "               testAccSamples.append(tacc)\n",
        "               trainAccSamples.append(acc)\n",
        "               testLossSamples.append(tloss)\n",
        "               trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "               # Clear the current state of the metrics\n",
        "               train_loss.reset_states(), train_acc.reset_states()\n",
        "               test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "               # Local logging\n",
        "               template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "               output = PrintFunc(template)\n",
        "               print(output)   \n",
        "               \n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "       #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "       testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "       #Appending fold mean values to arrays\n",
        "       MeantestAccSamples.append(testAccSamples)\n",
        "       MeantrainAccSamples.append(trainAccSamples)\n",
        "       MeantestLossSamples.append(testLossSamples)\n",
        "       MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "       #Resetting Arrays to capture values for next run\n",
        "       testAccSamples =[]\n",
        "       trainAccSamples = []\n",
        "       testLossSamples =[]\n",
        "       trainLossSamples = []\n",
        "\n",
        "       #Changing seed values for each run\n",
        "       seed = run + 1\n",
        "       tf.random.set_seed(seed)\n",
        "       np.random.seed(seed)\n",
        "       random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    #flattening the Mean values from all runs into a one dimensional array\n",
        "    testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "    testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "    trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "    trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "    testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "    testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "    trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "    trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "    #Calculating the standard deviation \n",
        "    atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "    atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "    atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "    atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "    #Calculating the mean across epochs for the multiple runs\n",
        "    atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "    ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "    atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "    ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "    file = open(\"MnistData.csv\",'a')  \n",
        "    file.write(\"CrossEntropy:\"+str(CE_SCALAR) + \" MeanSquared:\"+str(MSE_SCALAR))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n') \n",
        "    file.write(\"Acc Samples:\")\n",
        "    file.write('\\n')\n",
        "    for sample in testAccflatten[epoch]: \n",
        "        file.write(str(sample))\n",
        "        file.write('\\n')\n",
        "    file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n') \n",
        "    file.write(\"Loss Samples:\")\n",
        "    file.write('\\n')\n",
        "    for sample in testLossflatten[epoch]:        \n",
        "        file.write(str(sample))\n",
        "        file.write('\\n')\n",
        "    file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.close()  \n",
        "\n",
        "\n",
        "    StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "    x = createList(0,NUM_EPOCHS-1)\n",
        "    plt.clf()\n",
        "    plt.ylabel(\"Acc\", fontsize=14)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "    plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True,color='k')\n",
        "    #plt.show() \n",
        "    plt.savefig(f\"CE{CE_SCALAR}MSE{MSE_SCALAR}MnistAcc.eps\", bbox_inches='tight')\n",
        "    plt.clf()\n",
        "\n",
        "    plt.ylabel(\"Loss\", fontsize=14)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "    plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True,color='k')\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"CE{CE_SCALAR}MSE{MSE_SCALAR}MnistLoss.eps\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "    #Changing the hybrid for next run\n",
        "    CE_SCALAR = CE_SCALAR - 0.25\n",
        "    MSE_SCALAR = MSE_SCALAR + 0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDAT0kqmFCIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adaptive hybrid CE to MSE\n",
        "seed =1\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "CE_SCALAR = 1\n",
        "MSE_SCALAR = 0\n",
        "\n",
        "#Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "testAccSamples =[]\n",
        "trainAccSamples = []\n",
        "testLossSamples =[]\n",
        "trainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across folds\n",
        "MeantestAccSamples =[]\n",
        "MeantrainAccSamples = []\n",
        "MeantestLossSamples =[]\n",
        "MeantrainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across runs\n",
        "atr_accuracy_results = []\n",
        "ate_accuracy_results = []\n",
        "atr_loss_results = []\n",
        "ate_loss_results = []\n",
        "\n",
        "#Defining arrays to be used to flatten values into a one dimensional array \n",
        "testAccflatten = []\n",
        "trainAccflatten = []\n",
        "testLossflatten = []\n",
        "trainLossflatten = []\n",
        "\n",
        "#arrays used to capture standard deviation \n",
        "atrstdacc = []\n",
        "atestdacc = []\n",
        "atrstdloss = []\n",
        "atestdloss = []\n",
        "\n",
        "seed =1\n",
        "\n",
        "for run in range(RUNS):\n",
        "\n",
        "     \n",
        "\n",
        "    CEscalar = CE_SCALAR\n",
        "    MSEscalar = MSE_SCALAR\n",
        "\n",
        "  \n",
        "    #Constructing the model \n",
        "    model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "                              tf.keras.layers.Dense(10,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(10, activation='softmax'),\n",
        "       ])\n",
        "\n",
        "\n",
        "    #Save weights \n",
        "    Wsave = model.get_weights()\n",
        "    print(\"run {:d}\".format(run+1))\n",
        "\n",
        "    #Creating dataset for K-fold\n",
        "    #dataset = ut.make_dataset(features,labels,NUM_FOLDS)\n",
        "    fold=1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "\n",
        "    #For data in number of folds \n",
        "    for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "        X_train, X_test = features[train], features[test]\n",
        "        y_train, y_test= labels[train], labels[test]  \n",
        "\n",
        "        CEscalar = CE_SCALAR\n",
        "        MSEscalar = MSE_SCALAR  \n",
        "\n",
        "        #set saved weights\n",
        "        model.set_weights(Wsave)\n",
        "       \n",
        "        print(\"Fold {:d}\".format(fold))\n",
        "        fold = fold + 1 \n",
        "           \n",
        "        #train for a number of epochs\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "\n",
        "            tr = list(zip(X_train, y_train))\n",
        "            te = list(zip(X_test, y_test))\n",
        "            np.random.shuffle(tr)\n",
        "            np.random.shuffle(te)\n",
        "            X_train,y_train = zip(*tr) \n",
        "            X_test,y_test = zip(*te) \n",
        "\n",
        "            X_train=np.asarray(X_train, dtype=np.float64)\n",
        "            y_train=np.asarray(y_train, dtype=np.float64)\n",
        "            X_test=np.asarray(X_test, dtype=np.float64)\n",
        "            y_test=np.asarray(y_test, dtype=np.float64)\n",
        "\n",
        "\n",
        "            # Train on batch\n",
        "            for i in range(0,len(X_train) // BATCH_SIZE):\n",
        "                X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                gradients=model_train(X, y,model,CEscalar,MSEscalar,optimizer)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "            # Evaluate for each epoch    \n",
        "            model_test(X_test, y_test,model,CEscalar,MSEscalar)   \n",
        "                 \n",
        "\n",
        "            # Grab the results\n",
        "            (loss, acc) = train_loss.result(), train_acc.result()\n",
        "            (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "            #Appending values for each epoch \n",
        "            testAccSamples.append(tacc)\n",
        "            trainAccSamples.append(acc)\n",
        "            testLossSamples.append(tloss)\n",
        "            trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "            #Clear the current state of the metrics\n",
        "            train_loss.reset_states(), train_acc.reset_states()\n",
        "            test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "            #Local logging\n",
        "            template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "            output = PrintFunc(template)\n",
        "            print(output)   \n",
        "            CEscalar = CEscalar-0.01\n",
        "            MSEscalar = MSEscalar + 0.01\n",
        "            \n",
        "            print('CE:%.3f  MSE:%.3f' % (CEscalar, MSEscalar))\n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "    #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "    testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "    #Appending fold mean values to arrays\n",
        "    MeantestAccSamples.append(testAccSamples)\n",
        "    MeantrainAccSamples.append(trainAccSamples)\n",
        "    MeantestLossSamples.append(testLossSamples)\n",
        "    MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "    #Resetting Arrays to capture values for next run\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Changing seed values for each run\n",
        "    seed = run + 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#flattening the Mean values from all runs into a one dimensional array\n",
        "testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the standard deviation \n",
        "atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "#Calculating the mean across epochs for the multiple runs\n",
        "atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/MnistDataAdCEtoMSE.csv\",'a')  \n",
        "file.write(\"CEtoMSE\")\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Acc Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testAccflatten[epoch]: \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Loss Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testLossflatten[epoch]:        \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.close()  \n",
        "\n",
        "\n",
        "StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "x = createList(0,NUM_EPOCHS-1)\n",
        "plt.clf()\n",
        "plt.ylabel(\"Acc\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True,color='k')\n",
        "#plt.show() \n",
        "plt.savefig(f\"/content/AccMnistAdaptiveCEtoMSE.eps\", bbox_inches='tight')\n",
        "plt.clf()\n",
        "\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True,color='k')\n",
        "#plt.show()\n",
        "plt.savefig(f\"/content/LossMnistAdaptiveCEtoMSE.eps\", bbox_inches='tight')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl_F8NutFrCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adaptive hybrid MSE to CE\n",
        "seed =1\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "CE_SCALAR = 0\n",
        "MSE_SCALAR = 1\n",
        "\n",
        "#Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "testAccSamples =[]\n",
        "trainAccSamples = []\n",
        "testLossSamples =[]\n",
        "trainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across folds\n",
        "MeantestAccSamples =[]\n",
        "MeantrainAccSamples = []\n",
        "MeantestLossSamples =[]\n",
        "MeantrainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across runs\n",
        "atr_accuracy_results = []\n",
        "ate_accuracy_results = []\n",
        "atr_loss_results = []\n",
        "ate_loss_results = []\n",
        "\n",
        "#Defining arrays to be used to flatten values into a one dimensional array \n",
        "testAccflatten = []\n",
        "trainAccflatten = []\n",
        "testLossflatten = []\n",
        "trainLossflatten = []\n",
        "\n",
        "#arrays used to capture standard deviation \n",
        "atrstdacc = []\n",
        "atestdacc = []\n",
        "atrstdloss = []\n",
        "atestdloss = []\n",
        "\n",
        "\n",
        "\n",
        "for run in tf.range(RUNS):\n",
        "\n",
        "     \n",
        "\n",
        "    CEscalar = CE_SCALAR\n",
        "    MSEscalar = MSE_SCALAR\n",
        "\n",
        "  \n",
        "   \n",
        "    #Constructing the model \n",
        "    model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "                              tf.keras.layers.Dense(10,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(10, activation='softmax'),\n",
        "       ])\n",
        "\n",
        "\n",
        "\n",
        "    #Save weights \n",
        "    Wsave = model.get_weights()\n",
        "    print(\"run {:d}\".format(run+1))\n",
        "\n",
        "    #Creating dataset for K-fold\n",
        "    #dataset = ut.make_dataset(features,labels,NUM_FOLDS)\n",
        "    fold=1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "\n",
        "    #For data in number of folds \n",
        "    for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "        X_train, X_test = features[train], features[test]\n",
        "        y_train, y_test= labels[train], labels[test]  \n",
        "\n",
        "        CEscalar = CE_SCALAR\n",
        "        MSEscalar = MSE_SCALAR  \n",
        "\n",
        "        #set saved weights\n",
        "        model.set_weights(Wsave)\n",
        "       \n",
        "        print(\"Fold {:d}\".format(fold))\n",
        "        fold = fold + 1 \n",
        "           \n",
        "        #train for a number of epochs\n",
        "        for epoch in tf.range(NUM_EPOCHS):\n",
        "\n",
        "\n",
        "            tr = list(zip(X_train, y_train))\n",
        "            te = list(zip(X_test, y_test))\n",
        "            np.random.shuffle(tr)\n",
        "            np.random.shuffle(te)\n",
        "            X_train,y_train = zip(*tr) \n",
        "            X_test,y_test = zip(*te) \n",
        "\n",
        "            X_train=np.asarray(X_train, dtype=np.float64)\n",
        "            y_train=np.asarray(y_train, dtype=np.float64)\n",
        "            X_test=np.asarray(X_test, dtype=np.float64)\n",
        "            y_test=np.asarray(y_test, dtype=np.float64)\n",
        "\n",
        "\n",
        "            # Train on batch\n",
        "            for i in tf.range(0,len(X_train) // BATCH_SIZE):\n",
        "                X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                gradients=model_train(X, y,model,CEscalar,MSEscalar,optimizer)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "            # Evaluate for each epoch    \n",
        "            model_test(X_test, y_test,model,CEscalar,MSEscalar)   \n",
        "                 \n",
        "\n",
        "            # Grab the results\n",
        "            (loss, acc) = train_loss.result(), train_acc.result()\n",
        "            (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "            #Appending values for each epoch \n",
        "            testAccSamples.append(tacc)\n",
        "            trainAccSamples.append(acc)\n",
        "            testLossSamples.append(tloss)\n",
        "            trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "            #Clear the current state of the metrics\n",
        "            train_loss.reset_states(), train_acc.reset_states()\n",
        "            test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "            #Local logging\n",
        "            template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "            output = PrintFunc(template)\n",
        "            print(output)   \n",
        "            CEscalar = CEscalar+0.01\n",
        "            MSEscalar = MSEscalar - 0.01\n",
        "            \n",
        "            print('CE:%.3f  MSE:%.3f' % (CEscalar, MSEscalar))\n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "    #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "    testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "    #Appending fold mean values to arrays\n",
        "    MeantestAccSamples.append(testAccSamples)\n",
        "    MeantrainAccSamples.append(trainAccSamples)\n",
        "    MeantestLossSamples.append(testLossSamples)\n",
        "    MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "    #Resetting Arrays to capture values for next run\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Changing seed values for each run\n",
        "    seed = run + 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#flattening the Mean values from all runs into a one dimensional array\n",
        "testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the standard deviation \n",
        "atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "#Calculating the mean across epochs for the multiple runs\n",
        "atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/MnistDataAdMSEtoCE.csv\",'a')  \n",
        "file.write(\"MSEtoCE\")\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Acc Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testAccflatten[epoch]: \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Loss Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testLossflatten[epoch]:        \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.close()  \n",
        "\n",
        "\n",
        "StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "x = createList(0,NUM_EPOCHS-1)\n",
        "plt.clf()\n",
        "plt.ylabel(\"Acc\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True,color='k')\n",
        "#plt.show() \n",
        "plt.savefig(f\"/content/AccMnistAdaptiveMSEtoCE.eps\", bbox_inches='tight')\n",
        "plt.clf()\n",
        "\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True,color='k')\n",
        "#plt.show()\n",
        "plt.savefig(f\"/content/LossMnistAdaptiveMSEtoCE.eps\", bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBwxd_1ek7pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(StatsforHybrids)\n",
        "Hybrids = [\"CE1MSE0\",\"CE75MSE25\",\"CE5MSE5\",\"CE25MSE75\",\"CE0MSE1\",\"CEtoMSE\",\"MSEtoCE\"]\n",
        "whitney(StatsforHybrids,str1,0.05,alt=None,savetoFile=True,file=\"MnistStats.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
