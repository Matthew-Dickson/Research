{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA_hLy8hUMYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "6253e86d-0968-4c42-f8da-c37c01e7a38f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWxohmYRVBJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cc1dad4-a0bb-47d9-a81e-4ba9168b8aa6"
      },
      "source": [
        "\n",
        "%cd /content/gdrive/My Drive/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EyUv5jgzoUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn.preprocessing as pp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.datasets as sk\n",
        "import csv\n",
        "import math\n",
        "from decimal import Decimal\n",
        "import random\n",
        "from scipy.stats import mannwhitneyu\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.datasets import fetch_openml\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bsr_TT9lBh4J",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def cross_entropy(logits, y):\n",
        "    ce_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\n",
        "    return ce_op\n",
        "\n",
        "@tf.function\n",
        "def mse(pred, y):\n",
        "    mse_op = tf.reduce_mean(tf.square(pred - y))\n",
        "    return mse_op\n",
        "\n",
        "@tf.function\n",
        "def loss_function(labels,predictions,CEscalar,MSEscalar):\n",
        "\n",
        "  labels = tf.dtypes.cast(labels,tf.float64)\n",
        " \n",
        "  CE_object = cross_entropy(predictions,labels)\n",
        "\n",
        "  MSE_object = mse(predictions,labels)\n",
        " \n",
        " \n",
        "  loss_object = (CEscalar*CE_object)+(MSEscalar*MSE_object)\n",
        "\n",
        "  return loss_object\n",
        "\n",
        "@tf.function\n",
        "def model_test(features, labels,model,CEscalar,MSEscalar):\n",
        "    predictions = model(features,training=False)\n",
        "    t_loss = loss_function(labels, predictions, CEscalar, MSEscalar)\n",
        "    test_loss(t_loss)\n",
        "    test_acc(labels, predictions)\n",
        "    \n",
        "\n",
        "\n",
        "@tf.function\n",
        "def model_train(features, labels,model,CEscalar,MSEscalar,optimizer):\n",
        "    # Define the GradientTape context\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get the probabilities\n",
        "        predictions = model(features, training=True)\n",
        "        # Calculate the loss\n",
        "        loss = loss_function(labels, predictions, CEscalar, MSEscalar)   \n",
        "    # Get the gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "\n",
        "    # Update the loss and accuracy\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, predictions)\n",
        "    return gradients \n",
        "       \n",
        "\n",
        "      \n",
        "def PrintFunc(template):\n",
        "    return template.format(epoch+1,loss, acc,tloss,tacc)\n",
        "\n",
        "def shuffle(features,labels):\n",
        "    #zip data\n",
        "    data = list(zip(features,labels))\n",
        "\n",
        "    #shuffle data\n",
        "    #random.seed(1)\n",
        "    #random.shuffle(data)\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    #unzip data\n",
        "    features,labels = zip(*data)\n",
        "    features = np.asarray(features)\n",
        "    labels = np.asarray(labels)\n",
        "    return features,labels  \n",
        "\n",
        "\n",
        "\n",
        "def MeanAcrossEpoch(arr,num_epoch):\n",
        "    \n",
        "    tot = []\n",
        "   \n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    for i in arr:\n",
        "        val = 0\n",
        "        val = np.mean(i,dtype=np.float64)\n",
        "        tot.append(val)\n",
        "    return tot      \n",
        "\n",
        "\n",
        "def StdAcrossEpoch(arr,num_epoch):\n",
        "    \n",
        "    tot = []\n",
        "   \n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    for i in arr:\n",
        "        val = 0\n",
        "        val = np.std(i,dtype=np.float64,ddof=1)\n",
        "        tot.append(val)\n",
        "    return tot    \n",
        "\n",
        "\n",
        "def createList(r1, r2): \n",
        "  \n",
        "    # Testing if range r1 and r2  \n",
        "    # are equal \n",
        "    if (r1 == r2): \n",
        "        return r1 \n",
        "  \n",
        "    else: \n",
        "  \n",
        "        # Create empty list \n",
        "        res = [] \n",
        "  \n",
        "        # loop to append successors to  \n",
        "        # list until r2 is reached. \n",
        "        while(r1 < r2+1 ): \n",
        "              \n",
        "            res.append(r1) \n",
        "            r1 += 1\n",
        "        return res \n",
        "          \n",
        "\n",
        "def SameEpoch(arr,num_epoch):\n",
        "    new = []\n",
        "    for i in range(num_epoch):\n",
        "        new.append(arr[i::num_epoch])   \n",
        "    return new\n",
        "\n",
        "#Get the mean for each epoch across the different folds\n",
        "def MeanAcrossFolds(arr,num_epoch,num_folds):\n",
        "    tot = []\n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    \n",
        "    for i in range(num_epoch):\n",
        "        val = 0\n",
        "        val = np.sum(arr[i],dtype=np.float64)/num_folds\n",
        "        tot.append(val)\n",
        "    return tot       \n",
        "\n",
        "\n",
        "\n",
        "#Whitney U test\n",
        "def whitney(data,names,alpha=0.05,alt=None,savetoFile = False,file='default'):\n",
        "\n",
        "  if savetoFile == False:\n",
        "\n",
        "    for x in range(len(data)-1):\n",
        "      print('\\n')\n",
        "      for y in range(x+1,len(data)):\n",
        "\n",
        "        print(names[x] + ' Mean is less than ' + names[y])\n",
        "        stat, p = mannwhitneyu(data[x], data[y] , alternative= alt)\n",
        "        print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "        if p > alpha:\n",
        "\t        print('Same distribution (fail to reject H0)')\n",
        "        else:\n",
        "\t        print('Different distribution (reject H0)')\n",
        "  else:\n",
        "      file = open(\"C:/Users/AutoMAttic/Desktop/honours/COS700/Research/glass\"+file,'a')\n",
        "      for x in range(len(data)-1):\n",
        "        file.write('\\n')\n",
        "        file.write('\\n')\n",
        "        for y in range(x+1,len(data)):\n",
        "          file.write(names[x] + ' Checking distibution ' + names[y])\n",
        "          file.write('\\n')\n",
        "          stat, p = mannwhitneyu(data[x], data[y] , alternative= alt)\n",
        "          file.write('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "          file.write('\\n')\n",
        "          if p > alpha:\n",
        "\t          file.write('Same distribution (fail to reject H0)')\n",
        "          else:\n",
        "\t          file.write('Different distribution (reject H0)')     \n",
        "      file.close()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScnblEuCA4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting Environment\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "#random.seed(1)\n",
        "\n",
        "\n",
        "# Hyper Parameter\n",
        "LEARNING_RATE = 0.001\n",
        "CE_SCALAR = 1\n",
        "MSE_SCALAR = 0\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 15\n",
        "NUM_FOLDS = 10\n",
        "RUNS = 30\n",
        "TOTALEPOCHS = NUM_FOLDS*NUM_EPOCHS*RUNS\n",
        "\n",
        "#Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "train_acc = tf.keras.metrics.CategoricalAccuracy(name=\"train_acc\")\n",
        "test_acc = tf.keras.metrics.CategoricalAccuracy(name=\"test_acc\")\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Load features and labels from iris\n",
        "features, labels = sk.load_iris(return_X_y=True)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(features)\n",
        "features = scaler.transform(features)\n",
        "labels = pp.label_binarize(labels, classes=np.unique(labels))\n",
        "\n",
        "# shuffle data\n",
        "features,labels=shuffle(features,labels)\n",
        "\n",
        "#for stats test\n",
        "StatsforHybrids = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-irwq4dCX-V",
        "colab": {}
      },
      "source": [
        "\n",
        "#static hybrid functions\n",
        "while(CE_SCALAR >= 0):\n",
        "\n",
        "    seed = 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "\n",
        "    #Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Declaring arrays for holding mean values across folds\n",
        "    MeantestAccSamples =[]\n",
        "    MeantrainAccSamples = []\n",
        "    MeantestLossSamples =[]\n",
        "    MeantrainLossSamples = []\n",
        "\n",
        "    #Declaring arrays for holding mean values across runs\n",
        "    atr_accuracy_results = []\n",
        "    ate_accuracy_results = []\n",
        "    atr_loss_results = []\n",
        "    ate_loss_results = []\n",
        "\n",
        "    #Defining arrays to be used to flatten values into a one dimensional array \n",
        "    testAccflatten = []\n",
        "    trainAccflatten = []\n",
        "    testLossflatten = []\n",
        "    trainLossflatten = []\n",
        "\n",
        "    #arrays used to capture standard deviation \n",
        "    atrstdacc = []\n",
        "    atestdacc = []\n",
        "    atrstdloss = []\n",
        "    atestdloss = []\n",
        "\n",
        "\n",
        "    for run in tf.range(RUNS):\n",
        "\n",
        "     \n",
        "       \n",
        "\n",
        "       #Constructing the model \n",
        "       model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(4,)),\n",
        "                              tf.keras.layers.Dense(4,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(3, activation='softmax'),\n",
        "       ])\n",
        "\n",
        "\n",
        "\n",
        "       #Save weights \n",
        "       Wsave = model.get_weights()\n",
        "       print(\"run {:d}\".format(run+1))\n",
        "\n",
        "       \n",
        "       fold=1 \n",
        "\n",
        "       kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "       # For data in number of folds\n",
        "       for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "           X_train, X_test = features[train], features[test]\n",
        "           y_train, y_test= labels[train], labels[test]    \n",
        "           \n",
        "           #set saved weights\n",
        "           model.set_weights(Wsave) \n",
        "           \n",
        "           print(\"Fold {:d}\".format(fold))\n",
        "           fold = fold + 1 \n",
        "           \n",
        "           #train for a number of epochs\n",
        "           for epoch in tf.range(NUM_EPOCHS):\n",
        "               \n",
        "               #Shuffle dataset for each epoch\n",
        "               tr = list(zip(X_train, y_train))\n",
        "               te = list(zip(X_test, y_test))\n",
        "               np.random.shuffle(tr)\n",
        "               np.random.shuffle(te)\n",
        "               X_train,y_train = zip(*tr) \n",
        "               X_test,y_test = zip(*te) \n",
        "\n",
        "\n",
        "               #Make numpy arrays to feed into model\n",
        "               X_train=np.asarray(X_train, dtype=np.float64)\n",
        "               y_train=np.asarray(y_train, dtype=np.float64)\n",
        "               X_test=np.asarray(X_test, dtype=np.float64)\n",
        "               y_test=np.asarray(y_test, dtype=np.float64)\n",
        "               \n",
        "               \n",
        "               # Train on batch\n",
        "               for i in tf.range(0,len(X_train) // BATCH_SIZE):\n",
        "                   X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                   y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                   gradients=model_train(X, y,model,CE_SCALAR,MSE_SCALAR,optimizer)\n",
        "                   optimizer.apply_gradients(zip(gradients, model.trainable_weights))   \n",
        "                   \n",
        "                      \n",
        "\n",
        "               # Evaluate for each epoch    \n",
        "               model_test(X_test, y_test,model,CE_SCALAR,MSE_SCALAR)   \n",
        "\n",
        "               # Grab the results\n",
        "               (loss, acc) = train_loss.result(), train_acc.result()\n",
        "               (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "               #Appending values for each epoch \n",
        "               testAccSamples.append(tacc)\n",
        "               trainAccSamples.append(acc)\n",
        "               testLossSamples.append(tloss)\n",
        "               trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "               # Clear the current state of the metrics\n",
        "               train_loss.reset_states(), train_acc.reset_states()\n",
        "               test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "               # Local logging\n",
        "               template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "               output = PrintFunc(template)\n",
        "               print(output)   \n",
        "               \n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "       #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "       testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "       #Appending fold mean values to arrays\n",
        "       MeantestAccSamples.append(testAccSamples)\n",
        "       MeantrainAccSamples.append(trainAccSamples)\n",
        "       MeantestLossSamples.append(testLossSamples)\n",
        "       MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "       #Resetting Arrays to capture values for next run\n",
        "       testAccSamples =[]\n",
        "       trainAccSamples = []\n",
        "       testLossSamples =[]\n",
        "       trainLossSamples = []\n",
        "\n",
        "       #Changing seed values for each run\n",
        "       seed = run + 1\n",
        "       tf.random.set_seed(seed)\n",
        "       np.random.seed(seed)\n",
        "       #random.seed(seed)\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    #flattening the Mean values from all runs into a one dimensional array\n",
        "    testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "    testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "    trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "    trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "    testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "    testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "    trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "    trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "    #Calculating the standard deviation \n",
        "    atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "    atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "    atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "    atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "    #Calculating the mean across epochs for the multiple runs\n",
        "    atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "    ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "    atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "    ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "    file = open(\"IrisData.csv\",'a')  \n",
        "    file.write(\"CrossEntropy:\"+str(CE_SCALAR) + \" MeanSquared:\"+str(MSE_SCALAR))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n') \n",
        "    file.write(\"Acc Samples:\")\n",
        "    file.write('\\n')\n",
        "    for sample in testAccflatten[epoch]: \n",
        "        file.write(str(sample))\n",
        "        file.write('\\n')\n",
        "    file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n') \n",
        "    file.write(\"Loss Samples:\")\n",
        "    file.write('\\n')\n",
        "    for sample in testLossflatten[epoch]:        \n",
        "        file.write(str(sample))\n",
        "        file.write('\\n')\n",
        "    file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.close()  \n",
        "\n",
        "\n",
        "    StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "    x = createList(0,NUM_EPOCHS-1)\n",
        "    plt.clf()\n",
        "    plt.ylabel(\"Acc\", fontsize=14)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "    plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True,color='k')\n",
        "    #plt.show() \n",
        "    plt.savefig(f\"CE{SCALAR1}MSE{SCALAR2}IrisAcc.eps\", bbox_inches='tight')\n",
        "    plt.clf()\n",
        "\n",
        "    plt.ylabel(\"Loss\", fontsize=14)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "    plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True,color='k')\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"CE{CE_SCALAR}MSE{MSE_SCALAR}IrisLoss.eps\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "    #Changing the hybrid for next run\n",
        "    CE_SCALAR = CE_SCALAR - 0.25\n",
        "    MSE_SCALAR = MSE_SCALAR + 0.25\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRVpxgp32AE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8fcf316-bd0b-4844-b1d6-fae9eb79af16"
      },
      "source": [
        "  \n",
        "#Adaptive hybrid CE to MSE\n",
        "seed =1\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "CE_SCALAR = 1\n",
        "MSE_SCALAR = 0\n",
        "\n",
        "#Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "testAccSamples =[]\n",
        "trainAccSamples = []\n",
        "testLossSamples =[]\n",
        "trainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across folds\n",
        "MeantestAccSamples =[]\n",
        "MeantrainAccSamples = []\n",
        "MeantestLossSamples =[]\n",
        "MeantrainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across runs\n",
        "atr_accuracy_results = []\n",
        "ate_accuracy_results = []\n",
        "atr_loss_results = []\n",
        "ate_loss_results = []\n",
        "\n",
        "#Defining arrays to be used to flatten values into a one dimensional array \n",
        "testAccflatten = []\n",
        "trainAccflatten = []\n",
        "testLossflatten = []\n",
        "trainLossflatten = []\n",
        "\n",
        "#arrays used to capture standard deviation \n",
        "atrstdacc = []\n",
        "atestdacc = []\n",
        "atrstdloss = []\n",
        "atestdloss = []\n",
        "\n",
        "seed =1\n",
        "\n",
        "for run in range(RUNS):\n",
        "\n",
        "     \n",
        "\n",
        "    CEscalar = CE_SCALAR\n",
        "    MSEscalar = MSE_SCALAR\n",
        "\n",
        "  \n",
        "    #Constructing the model \n",
        "    model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(4,)),\n",
        "                              tf.keras.layers.Dense(4,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(3, activation='softmax'),\n",
        "       ])\n",
        "\n",
        "\n",
        "    #Save weights \n",
        "    Wsave = model.get_weights()\n",
        "    print(\"run {:d}\".format(run+1))\n",
        "\n",
        "    #Creating dataset for K-fold\n",
        "    #dataset = ut.make_dataset(features,labels,NUM_FOLDS)\n",
        "    fold=1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "\n",
        "    #For data in number of folds \n",
        "    for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "        X_train, X_test = features[train], features[test]\n",
        "        y_train, y_test= labels[train], labels[test]  \n",
        "\n",
        "        CEscalar = CE_SCALAR\n",
        "        MSEscalar = MSE_SCALAR  \n",
        "\n",
        "        #set saved weights\n",
        "        model.set_weights(Wsave)\n",
        "       \n",
        "        print(\"Fold {:d}\".format(fold))\n",
        "        fold = fold + 1 \n",
        "           \n",
        "        #train for a number of epochs\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "\n",
        "            tr = list(zip(X_train, y_train))\n",
        "            te = list(zip(X_test, y_test))\n",
        "            np.random.shuffle(tr)\n",
        "            np.random.shuffle(te)\n",
        "            X_train,y_train = zip(*tr) \n",
        "            X_test,y_test = zip(*te) \n",
        "\n",
        "            X_train=np.asarray(X_train, dtype=np.float64)\n",
        "            y_train=np.asarray(y_train, dtype=np.float64)\n",
        "            X_test=np.asarray(X_test, dtype=np.float64)\n",
        "            y_test=np.asarray(y_test, dtype=np.float64)\n",
        "\n",
        "\n",
        "            # Train on batch\n",
        "            for i in range(0,len(X_train) // BATCH_SIZE):\n",
        "                X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                gradients=model_train(X, y,model,CEscalar,MSEscalar,optimizer)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "            # Evaluate for each epoch    \n",
        "            model_test(X_test, y_test,model,CEscalar,MSEscalar)   \n",
        "                 \n",
        "\n",
        "            # Grab the results\n",
        "            (loss, acc) = train_loss.result(), train_acc.result()\n",
        "            (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "            #Appending values for each epoch \n",
        "            testAccSamples.append(tacc)\n",
        "            trainAccSamples.append(acc)\n",
        "            testLossSamples.append(tloss)\n",
        "            trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "            #Clear the current state of the metrics\n",
        "            train_loss.reset_states(), train_acc.reset_states()\n",
        "            test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "            #Local logging\n",
        "            template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "            output = PrintFunc(template)\n",
        "            print(output)   \n",
        "            CEscalar = CEscalar-0.01\n",
        "            MSEscalar = MSEscalar + 0.01\n",
        "            \n",
        "            print('CE:%.3f  MSE:%.3f' % (CEscalar, MSEscalar))\n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "    #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "    testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "    #Appending fold mean values to arrays\n",
        "    MeantestAccSamples.append(testAccSamples)\n",
        "    MeantrainAccSamples.append(trainAccSamples)\n",
        "    MeantestLossSamples.append(testLossSamples)\n",
        "    MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "    #Resetting Arrays to capture values for next run\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Changing seed values for each run\n",
        "    seed = run + 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#flattening the Mean values from all runs into a one dimensional array\n",
        "testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the standard deviation \n",
        "atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "#Calculating the mean across epochs for the multiple runs\n",
        "atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/IrisDataAdCEtoMSE.csv\",'a')  \n",
        "file.write(\"CEtoMSE\")\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Acc Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testAccflatten[epoch]: \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Loss Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testLossflatten[epoch]:        \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.close()  \n",
        "\n",
        "\n",
        "StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "x = createList(0,NUM_EPOCHS-1)\n",
        "plt.clf()\n",
        "plt.ylabel(\"Acc\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True,color='k')\n",
        "#plt.show() \n",
        "plt.savefig(f\"/content/AccIrisAdaptiveCEtoMSE.eps\", bbox_inches='tight')\n",
        "plt.clf()\n",
        "\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True,color='k')\n",
        "#plt.show()\n",
        "plt.savefig(f\"/content/LossIrisAdaptiveCEtoMSE.eps\", bbox_inches='tight')\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run 1\n",
            "Fold 1\n",
            "Epoch 001, loss: 0.995, acc: 0.561, test_loss 1.152, test_acc 0.333\n",
            "CE:0.990  MSE:0.010\n",
            "Epoch 002, loss: 1.145, acc: 0.333, test_loss 1.095, test_acc 0.333\n",
            "CE:0.980  MSE:0.020\n",
            "Epoch 003, loss: 1.083, acc: 0.333, test_loss 1.032, test_acc 0.400\n",
            "CE:0.970  MSE:0.030\n",
            "Epoch 004, loss: 1.022, acc: 0.422, test_loss 0.974, test_acc 0.533\n",
            "CE:0.960  MSE:0.040\n",
            "Epoch 005, loss: 0.963, acc: 0.556, test_loss 0.929, test_acc 0.667\n",
            "CE:0.950  MSE:0.050\n",
            "Epoch 006, loss: 0.920, acc: 0.637, test_loss 0.893, test_acc 0.533\n",
            "CE:0.940  MSE:0.060\n",
            "Epoch 007, loss: 0.885, acc: 0.644, test_loss 0.862, test_acc 0.600\n",
            "CE:0.930  MSE:0.070\n",
            "Epoch 008, loss: 0.854, acc: 0.667, test_loss 0.834, test_acc 0.600\n",
            "CE:0.920  MSE:0.080\n",
            "Epoch 009, loss: 0.825, acc: 0.696, test_loss 0.807, test_acc 0.600\n",
            "CE:0.910  MSE:0.090\n",
            "Epoch 010, loss: 0.800, acc: 0.711, test_loss 0.783, test_acc 0.733\n",
            "CE:0.900  MSE:0.100\n",
            "Epoch 011, loss: 0.779, acc: 0.741, test_loss 0.762, test_acc 0.733\n",
            "CE:0.890  MSE:0.110\n",
            "Epoch 012, loss: 0.758, acc: 0.733, test_loss 0.743, test_acc 0.733\n",
            "CE:0.880  MSE:0.120\n",
            "Epoch 013, loss: 0.741, acc: 0.741, test_loss 0.725, test_acc 0.733\n",
            "CE:0.870  MSE:0.130\n",
            "Epoch 014, loss: 0.725, acc: 0.763, test_loss 0.709, test_acc 0.733\n",
            "CE:0.860  MSE:0.140\n",
            "Epoch 015, loss: 0.710, acc: 0.770, test_loss 0.693, test_acc 0.733\n",
            "CE:0.850  MSE:0.150\n",
            "Epoch 016, loss: 0.697, acc: 0.778, test_loss 0.679, test_acc 0.800\n",
            "CE:0.840  MSE:0.160\n",
            "Epoch 017, loss: 0.685, acc: 0.778, test_loss 0.666, test_acc 0.800\n",
            "CE:0.830  MSE:0.170\n",
            "Epoch 018, loss: 0.674, acc: 0.785, test_loss 0.654, test_acc 0.800\n",
            "CE:0.820  MSE:0.180\n",
            "Epoch 019, loss: 0.663, acc: 0.785, test_loss 0.644, test_acc 0.800\n",
            "CE:0.810  MSE:0.190\n",
            "Epoch 020, loss: 0.652, acc: 0.785, test_loss 0.633, test_acc 0.800\n",
            "CE:0.800  MSE:0.200\n",
            "Epoch 021, loss: 0.642, acc: 0.785, test_loss 0.623, test_acc 0.800\n",
            "CE:0.790  MSE:0.210\n",
            "Epoch 022, loss: 0.632, acc: 0.785, test_loss 0.614, test_acc 0.800\n",
            "CE:0.780  MSE:0.220\n",
            "Epoch 023, loss: 0.622, acc: 0.785, test_loss 0.604, test_acc 0.800\n",
            "CE:0.770  MSE:0.230\n",
            "Epoch 024, loss: 0.613, acc: 0.785, test_loss 0.595, test_acc 0.800\n",
            "CE:0.760  MSE:0.240\n",
            "Epoch 025, loss: 0.603, acc: 0.785, test_loss 0.587, test_acc 0.800\n",
            "CE:0.750  MSE:0.250\n",
            "Epoch 026, loss: 0.594, acc: 0.793, test_loss 0.579, test_acc 0.800\n",
            "CE:0.740  MSE:0.260\n",
            "Epoch 027, loss: 0.585, acc: 0.807, test_loss 0.571, test_acc 0.800\n",
            "CE:0.730  MSE:0.270\n",
            "Epoch 028, loss: 0.576, acc: 0.822, test_loss 0.563, test_acc 0.800\n",
            "CE:0.720  MSE:0.280\n",
            "Epoch 029, loss: 0.568, acc: 0.822, test_loss 0.555, test_acc 0.800\n",
            "CE:0.710  MSE:0.290\n",
            "Epoch 030, loss: 0.559, acc: 0.822, test_loss 0.547, test_acc 0.800\n",
            "CE:0.700  MSE:0.300\n",
            "Epoch 031, loss: 0.550, acc: 0.822, test_loss 0.539, test_acc 0.800\n",
            "CE:0.690  MSE:0.310\n",
            "Epoch 032, loss: 0.542, acc: 0.830, test_loss 0.532, test_acc 0.867\n",
            "CE:0.680  MSE:0.320\n",
            "Epoch 033, loss: 0.534, acc: 0.837, test_loss 0.524, test_acc 0.867\n",
            "CE:0.670  MSE:0.330\n",
            "Epoch 034, loss: 0.525, acc: 0.837, test_loss 0.517, test_acc 0.867\n",
            "CE:0.660  MSE:0.340\n",
            "Epoch 035, loss: 0.517, acc: 0.837, test_loss 0.509, test_acc 0.867\n",
            "CE:0.650  MSE:0.350\n",
            "Epoch 036, loss: 0.509, acc: 0.837, test_loss 0.501, test_acc 0.867\n",
            "CE:0.640  MSE:0.360\n",
            "Epoch 037, loss: 0.501, acc: 0.837, test_loss 0.494, test_acc 0.867\n",
            "CE:0.630  MSE:0.370\n",
            "Epoch 038, loss: 0.493, acc: 0.837, test_loss 0.486, test_acc 0.867\n",
            "CE:0.620  MSE:0.380\n",
            "Epoch 039, loss: 0.485, acc: 0.844, test_loss 0.479, test_acc 0.867\n",
            "CE:0.610  MSE:0.390\n",
            "Epoch 040, loss: 0.477, acc: 0.844, test_loss 0.471, test_acc 0.867\n",
            "CE:0.600  MSE:0.400\n",
            "Epoch 041, loss: 0.469, acc: 0.844, test_loss 0.464, test_acc 0.867\n",
            "CE:0.590  MSE:0.410\n",
            "Epoch 042, loss: 0.462, acc: 0.844, test_loss 0.457, test_acc 0.867\n",
            "CE:0.580  MSE:0.420\n",
            "Epoch 043, loss: 0.454, acc: 0.852, test_loss 0.449, test_acc 0.867\n",
            "CE:0.570  MSE:0.430\n",
            "Epoch 044, loss: 0.446, acc: 0.852, test_loss 0.442, test_acc 0.867\n",
            "CE:0.560  MSE:0.440\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bd437030f421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m               \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m               \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m               \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCEscalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMSEscalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7478faa876ed>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(features, labels, model, CEscalar, MSEscalar, optimizer)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Get the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCEscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSEscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    717\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mshape_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m       outputs = array_ops.reshape(\n\u001b[0;32m--> 652\u001b[0;31m           inputs, constant_op.constant((-1, flattened_dim), dtype=shape_dtype))\n\u001b[0m\u001b[1;32m    653\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m       \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8073\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   8074\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Reshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8075\u001b[0;31m         tld.op_callbacks, tensor, shape)\n\u001b[0m\u001b[1;32m   8076\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8077\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z80sS2eUIp7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5b405b0-0325-42bd-9887-0296dbba3007"
      },
      "source": [
        "#Adaptive hybrid MSE to CE\n",
        "seed =1\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "CE_SCALAR = 0\n",
        "MSE_SCALAR = 1\n",
        "\n",
        "#Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "testAccSamples =[]\n",
        "trainAccSamples = []\n",
        "testLossSamples =[]\n",
        "trainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across folds\n",
        "MeantestAccSamples =[]\n",
        "MeantrainAccSamples = []\n",
        "MeantestLossSamples =[]\n",
        "MeantrainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across runs\n",
        "atr_accuracy_results = []\n",
        "ate_accuracy_results = []\n",
        "atr_loss_results = []\n",
        "ate_loss_results = []\n",
        "\n",
        "#Defining arrays to be used to flatten values into a one dimensional array \n",
        "testAccflatten = []\n",
        "trainAccflatten = []\n",
        "testLossflatten = []\n",
        "trainLossflatten = []\n",
        "\n",
        "#arrays used to capture standard deviation \n",
        "atrstdacc = []\n",
        "atestdacc = []\n",
        "atrstdloss = []\n",
        "atestdloss = []\n",
        "\n",
        "seed =1\n",
        "\n",
        "for run in tf.range(RUNS):\n",
        "\n",
        "     \n",
        "\n",
        "    CEscalar = CE_SCALAR\n",
        "    MSEscalar = MSE_SCALAR\n",
        "\n",
        "  \n",
        "   \n",
        "    #Constructing the model \n",
        "    model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(4,)),\n",
        "                              tf.keras.layers.Dense(4,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(3, activation='softmax'),\n",
        "       ])\n",
        "\n",
        "\n",
        "\n",
        "    #Save weights \n",
        "    Wsave = model.get_weights()\n",
        "    print(\"run {:d}\".format(run+1))\n",
        "\n",
        "    #Creating dataset for K-fold\n",
        "    #dataset = ut.make_dataset(features,labels,NUM_FOLDS)\n",
        "    fold=1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "\n",
        "    #For data in number of folds \n",
        "    for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "        X_train, X_test = features[train], features[test]\n",
        "        y_train, y_test= labels[train], labels[test]  \n",
        "\n",
        "        CEscalar = CE_SCALAR\n",
        "        MSEscalar = MSE_SCALAR  \n",
        "\n",
        "        #set saved weights\n",
        "        model.set_weights(Wsave)\n",
        "       \n",
        "        print(\"Fold {:d}\".format(fold))\n",
        "        fold = fold + 1 \n",
        "           \n",
        "        #train for a number of epochs\n",
        "        for epoch in tf.range(NUM_EPOCHS):\n",
        "\n",
        "\n",
        "            tr = list(zip(X_train, y_train))\n",
        "            te = list(zip(X_test, y_test))\n",
        "            np.random.shuffle(tr)\n",
        "            np.random.shuffle(te)\n",
        "            X_train,y_train = zip(*tr) \n",
        "            X_test,y_test = zip(*te) \n",
        "\n",
        "            X_train=np.asarray(X_train, dtype=np.float64)\n",
        "            y_train=np.asarray(y_train, dtype=np.float64)\n",
        "            X_test=np.asarray(X_test, dtype=np.float64)\n",
        "            y_test=np.asarray(y_test, dtype=np.float64)\n",
        "\n",
        "\n",
        "            # Train on batch\n",
        "            for i in tf.range(0,len(X_train) // BATCH_SIZE):\n",
        "                X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                gradients=model_train(X, y,model,CEscalar,MSEscalar,optimizer)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "            # Evaluate for each epoch    \n",
        "            model_test(X_test, y_test,model,CEscalar,MSEscalar)   \n",
        "                 \n",
        "\n",
        "            # Grab the results\n",
        "            (loss, acc) = train_loss.result(), train_acc.result()\n",
        "            (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "            #Appending values for each epoch \n",
        "            testAccSamples.append(tacc)\n",
        "            trainAccSamples.append(acc)\n",
        "            testLossSamples.append(tloss)\n",
        "            trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "            #Clear the current state of the metrics\n",
        "            train_loss.reset_states(), train_acc.reset_states()\n",
        "            test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "            #Local logging\n",
        "            template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "            output = PrintFunc(template)\n",
        "            print(output)   \n",
        "            CEscalar = CEscalar+0.01\n",
        "            MSEscalar = MSEscalar - 0.01\n",
        "            \n",
        "            print('CE:%.3f  MSE:%.3f' % (CEscalar, MSEscalar))\n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "    #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "    testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "    #Appending fold mean values to arrays\n",
        "    MeantestAccSamples.append(testAccSamples)\n",
        "    MeantrainAccSamples.append(trainAccSamples)\n",
        "    MeantestLossSamples.append(testLossSamples)\n",
        "    MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "    #Resetting Arrays to capture values for next run\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Changing seed values for each run\n",
        "    seed = run + 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#flattening the Mean values from all runs into a one dimensional array\n",
        "testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the standard deviation \n",
        "atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "#Calculating the mean across epochs for the multiple runs\n",
        "atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/IrisDataAdMSEtoCE.csv\",'a')  \n",
        "file.write(\"MSEtoCE\")\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Acc Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testAccflatten[epoch]: \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Loss Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testLossflatten[epoch]:        \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.close()  \n",
        "\n",
        "\n",
        "StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "x = createList(0,NUM_EPOCHS-1)\n",
        "plt.clf()\n",
        "plt.ylabel(\"Acc\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True,color='k')\n",
        "#plt.show() \n",
        "plt.savefig(f\"/content/AccIrisAdaptiveMSEtoCE.eps\", bbox_inches='tight')\n",
        "plt.clf()\n",
        "\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True,color='k')\n",
        "#plt.show()\n",
        "plt.savefig(f\"/content/LossIrisAdaptiveMSEtoCE.eps\", bbox_inches='tight')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run 1\n",
            "Fold 1\n",
            "Epoch 001, loss: 0.347, acc: 0.613, test_loss 0.192, test_acc 0.400\n",
            "CE:0.010  MSE:0.990\n",
            "Epoch 002, loss: 0.188, acc: 0.615, test_loss 0.164, test_acc 0.867\n",
            "CE:0.020  MSE:0.980\n",
            "Epoch 003, loss: 0.163, acc: 0.748, test_loss 0.156, test_acc 0.667\n",
            "CE:0.030  MSE:0.970\n",
            "Epoch 004, loss: 0.155, acc: 0.763, test_loss 0.155, test_acc 0.667\n",
            "CE:0.040  MSE:0.960\n",
            "Epoch 005, loss: 0.152, acc: 0.785, test_loss 0.152, test_acc 0.733\n",
            "CE:0.050  MSE:0.950\n",
            "Epoch 006, loss: 0.150, acc: 0.800, test_loss 0.151, test_acc 0.800\n",
            "CE:0.060  MSE:0.940\n",
            "Epoch 007, loss: 0.151, acc: 0.807, test_loss 0.151, test_acc 0.800\n",
            "CE:0.070  MSE:0.930\n",
            "Epoch 008, loss: 0.152, acc: 0.822, test_loss 0.153, test_acc 0.800\n",
            "CE:0.080  MSE:0.920\n",
            "Epoch 009, loss: 0.155, acc: 0.837, test_loss 0.156, test_acc 0.800\n",
            "CE:0.090  MSE:0.910\n",
            "Epoch 010, loss: 0.158, acc: 0.837, test_loss 0.160, test_acc 0.800\n",
            "CE:0.100  MSE:0.900\n",
            "Epoch 011, loss: 0.161, acc: 0.844, test_loss 0.164, test_acc 0.800\n",
            "CE:0.110  MSE:0.890\n",
            "Epoch 012, loss: 0.166, acc: 0.844, test_loss 0.168, test_acc 0.867\n",
            "CE:0.120  MSE:0.880\n",
            "Epoch 013, loss: 0.170, acc: 0.852, test_loss 0.173, test_acc 0.867\n",
            "CE:0.130  MSE:0.870\n",
            "Epoch 014, loss: 0.175, acc: 0.852, test_loss 0.178, test_acc 0.867\n",
            "CE:0.140  MSE:0.860\n",
            "Epoch 015, loss: 0.180, acc: 0.859, test_loss 0.183, test_acc 0.867\n",
            "CE:0.150  MSE:0.850\n",
            "Epoch 016, loss: 0.184, acc: 0.867, test_loss 0.188, test_acc 0.867\n",
            "CE:0.160  MSE:0.840\n",
            "Epoch 017, loss: 0.190, acc: 0.867, test_loss 0.193, test_acc 0.867\n",
            "CE:0.170  MSE:0.830\n",
            "Epoch 018, loss: 0.195, acc: 0.867, test_loss 0.199, test_acc 0.867\n",
            "CE:0.180  MSE:0.820\n",
            "Epoch 019, loss: 0.200, acc: 0.867, test_loss 0.204, test_acc 0.867\n",
            "CE:0.190  MSE:0.810\n",
            "Epoch 020, loss: 0.205, acc: 0.867, test_loss 0.209, test_acc 0.867\n",
            "CE:0.200  MSE:0.800\n",
            "Epoch 021, loss: 0.211, acc: 0.867, test_loss 0.215, test_acc 0.867\n",
            "CE:0.210  MSE:0.790\n",
            "Epoch 022, loss: 0.216, acc: 0.867, test_loss 0.220, test_acc 0.867\n",
            "CE:0.220  MSE:0.780\n",
            "Epoch 023, loss: 0.221, acc: 0.867, test_loss 0.225, test_acc 0.867\n",
            "CE:0.230  MSE:0.770\n",
            "Epoch 024, loss: 0.227, acc: 0.867, test_loss 0.231, test_acc 0.867\n",
            "CE:0.240  MSE:0.760\n",
            "Epoch 025, loss: 0.232, acc: 0.867, test_loss 0.236, test_acc 0.867\n",
            "CE:0.250  MSE:0.750\n",
            "Epoch 026, loss: 0.238, acc: 0.867, test_loss 0.241, test_acc 0.867\n",
            "CE:0.260  MSE:0.740\n",
            "Epoch 027, loss: 0.243, acc: 0.874, test_loss 0.246, test_acc 0.867\n",
            "CE:0.270  MSE:0.730\n",
            "Epoch 028, loss: 0.248, acc: 0.874, test_loss 0.252, test_acc 0.867\n",
            "CE:0.280  MSE:0.720\n",
            "Epoch 029, loss: 0.254, acc: 0.874, test_loss 0.257, test_acc 0.867\n",
            "CE:0.290  MSE:0.710\n",
            "Epoch 030, loss: 0.259, acc: 0.874, test_loss 0.263, test_acc 0.867\n",
            "CE:0.300  MSE:0.700\n",
            "Epoch 031, loss: 0.265, acc: 0.874, test_loss 0.268, test_acc 0.867\n",
            "CE:0.310  MSE:0.690\n",
            "Epoch 032, loss: 0.270, acc: 0.889, test_loss 0.273, test_acc 0.867\n",
            "CE:0.320  MSE:0.680\n",
            "Epoch 033, loss: 0.275, acc: 0.889, test_loss 0.279, test_acc 0.867\n",
            "CE:0.330  MSE:0.670\n",
            "Epoch 034, loss: 0.281, acc: 0.889, test_loss 0.285, test_acc 0.867\n",
            "CE:0.340  MSE:0.660\n",
            "Epoch 035, loss: 0.286, acc: 0.889, test_loss 0.289, test_acc 0.867\n",
            "CE:0.350  MSE:0.650\n",
            "Epoch 036, loss: 0.291, acc: 0.889, test_loss 0.295, test_acc 0.867\n",
            "CE:0.360  MSE:0.640\n",
            "Epoch 037, loss: 0.297, acc: 0.889, test_loss 0.299, test_acc 0.867\n",
            "CE:0.370  MSE:0.630\n",
            "Epoch 038, loss: 0.302, acc: 0.896, test_loss 0.305, test_acc 0.867\n",
            "CE:0.380  MSE:0.620\n",
            "Epoch 039, loss: 0.307, acc: 0.896, test_loss 0.310, test_acc 0.867\n",
            "CE:0.390  MSE:0.610\n",
            "Epoch 040, loss: 0.312, acc: 0.896, test_loss 0.315, test_acc 0.867\n",
            "CE:0.400  MSE:0.600\n",
            "Epoch 041, loss: 0.318, acc: 0.896, test_loss 0.320, test_acc 0.867\n",
            "CE:0.410  MSE:0.590\n",
            "Epoch 042, loss: 0.323, acc: 0.919, test_loss 0.325, test_acc 0.867\n",
            "CE:0.420  MSE:0.580\n",
            "Epoch 043, loss: 0.328, acc: 0.911, test_loss 0.330, test_acc 0.933\n",
            "CE:0.430  MSE:0.570\n",
            "Epoch 044, loss: 0.333, acc: 0.919, test_loss 0.336, test_acc 0.933\n",
            "CE:0.440  MSE:0.560\n",
            "Epoch 045, loss: 0.338, acc: 0.926, test_loss 0.341, test_acc 0.933\n",
            "CE:0.450  MSE:0.550\n",
            "Epoch 046, loss: 0.343, acc: 0.919, test_loss 0.346, test_acc 0.933\n",
            "CE:0.460  MSE:0.540\n",
            "Epoch 047, loss: 0.348, acc: 0.933, test_loss 0.351, test_acc 0.933\n",
            "CE:0.470  MSE:0.530\n",
            "Epoch 048, loss: 0.353, acc: 0.933, test_loss 0.356, test_acc 0.933\n",
            "CE:0.480  MSE:0.520\n",
            "Epoch 049, loss: 0.359, acc: 0.933, test_loss 0.362, test_acc 0.933\n",
            "CE:0.490  MSE:0.510\n",
            "Epoch 050, loss: 0.363, acc: 0.941, test_loss 0.366, test_acc 0.933\n",
            "CE:0.500  MSE:0.500\n",
            "Epoch 051, loss: 0.369, acc: 0.941, test_loss 0.371, test_acc 0.933\n",
            "CE:0.510  MSE:0.490\n",
            "Epoch 052, loss: 0.374, acc: 0.941, test_loss 0.376, test_acc 0.933\n",
            "CE:0.520  MSE:0.480\n",
            "Epoch 053, loss: 0.379, acc: 0.941, test_loss 0.382, test_acc 0.933\n",
            "CE:0.530  MSE:0.470\n",
            "Epoch 054, loss: 0.384, acc: 0.941, test_loss 0.386, test_acc 0.933\n",
            "CE:0.540  MSE:0.460\n",
            "Epoch 055, loss: 0.389, acc: 0.941, test_loss 0.391, test_acc 0.933\n",
            "CE:0.550  MSE:0.450\n",
            "Epoch 056, loss: 0.394, acc: 0.948, test_loss 0.396, test_acc 0.933\n",
            "CE:0.560  MSE:0.440\n",
            "Epoch 057, loss: 0.399, acc: 0.948, test_loss 0.401, test_acc 0.933\n",
            "CE:0.570  MSE:0.430\n",
            "Epoch 058, loss: 0.404, acc: 0.948, test_loss 0.406, test_acc 0.933\n",
            "CE:0.580  MSE:0.420\n",
            "Epoch 059, loss: 0.409, acc: 0.948, test_loss 0.411, test_acc 0.933\n",
            "CE:0.590  MSE:0.410\n",
            "Epoch 060, loss: 0.414, acc: 0.948, test_loss 0.416, test_acc 0.933\n",
            "CE:0.600  MSE:0.400\n",
            "Epoch 061, loss: 0.418, acc: 0.948, test_loss 0.421, test_acc 0.933\n",
            "CE:0.610  MSE:0.390\n",
            "Epoch 062, loss: 0.424, acc: 0.941, test_loss 0.426, test_acc 1.000\n",
            "CE:0.620  MSE:0.380\n",
            "Epoch 063, loss: 0.428, acc: 0.948, test_loss 0.431, test_acc 1.000\n",
            "CE:0.630  MSE:0.370\n",
            "Epoch 064, loss: 0.434, acc: 0.956, test_loss 0.435, test_acc 1.000\n",
            "CE:0.640  MSE:0.360\n",
            "Epoch 065, loss: 0.438, acc: 0.956, test_loss 0.440, test_acc 1.000\n",
            "CE:0.650  MSE:0.350\n",
            "Epoch 066, loss: 0.443, acc: 0.948, test_loss 0.445, test_acc 1.000\n",
            "CE:0.660  MSE:0.340\n",
            "Epoch 067, loss: 0.448, acc: 0.948, test_loss 0.450, test_acc 1.000\n",
            "CE:0.670  MSE:0.330\n",
            "Epoch 068, loss: 0.453, acc: 0.948, test_loss 0.455, test_acc 1.000\n",
            "CE:0.680  MSE:0.320\n",
            "Epoch 069, loss: 0.458, acc: 0.948, test_loss 0.461, test_acc 1.000\n",
            "CE:0.690  MSE:0.310\n",
            "Epoch 070, loss: 0.463, acc: 0.948, test_loss 0.466, test_acc 1.000\n",
            "CE:0.700  MSE:0.300\n",
            "Epoch 071, loss: 0.468, acc: 0.948, test_loss 0.470, test_acc 1.000\n",
            "CE:0.710  MSE:0.290\n",
            "Epoch 072, loss: 0.473, acc: 0.948, test_loss 0.475, test_acc 1.000\n",
            "CE:0.720  MSE:0.280\n",
            "Epoch 073, loss: 0.478, acc: 0.948, test_loss 0.480, test_acc 1.000\n",
            "CE:0.730  MSE:0.270\n",
            "Epoch 074, loss: 0.483, acc: 0.948, test_loss 0.484, test_acc 1.000\n",
            "CE:0.740  MSE:0.260\n",
            "Epoch 075, loss: 0.488, acc: 0.948, test_loss 0.489, test_acc 1.000\n",
            "CE:0.750  MSE:0.250\n",
            "Epoch 076, loss: 0.493, acc: 0.948, test_loss 0.494, test_acc 1.000\n",
            "CE:0.760  MSE:0.240\n",
            "Epoch 077, loss: 0.498, acc: 0.956, test_loss 0.500, test_acc 1.000\n",
            "CE:0.770  MSE:0.230\n",
            "Epoch 078, loss: 0.503, acc: 0.956, test_loss 0.505, test_acc 1.000\n",
            "CE:0.780  MSE:0.220\n",
            "Epoch 079, loss: 0.508, acc: 0.956, test_loss 0.510, test_acc 1.000\n",
            "CE:0.790  MSE:0.210\n",
            "Epoch 080, loss: 0.513, acc: 0.956, test_loss 0.515, test_acc 1.000\n",
            "CE:0.800  MSE:0.200\n",
            "Epoch 081, loss: 0.518, acc: 0.956, test_loss 0.520, test_acc 1.000\n",
            "CE:0.810  MSE:0.190\n",
            "Epoch 082, loss: 0.524, acc: 0.956, test_loss 0.525, test_acc 1.000\n",
            "CE:0.820  MSE:0.180\n",
            "Epoch 083, loss: 0.528, acc: 0.956, test_loss 0.530, test_acc 1.000\n",
            "CE:0.830  MSE:0.170\n",
            "Epoch 084, loss: 0.534, acc: 0.956, test_loss 0.534, test_acc 1.000\n",
            "CE:0.840  MSE:0.160\n",
            "Epoch 085, loss: 0.538, acc: 0.956, test_loss 0.539, test_acc 1.000\n",
            "CE:0.850  MSE:0.150\n",
            "Epoch 086, loss: 0.544, acc: 0.956, test_loss 0.544, test_acc 1.000\n",
            "CE:0.860  MSE:0.140\n",
            "Epoch 087, loss: 0.549, acc: 0.956, test_loss 0.550, test_acc 1.000\n",
            "CE:0.870  MSE:0.130\n",
            "Epoch 088, loss: 0.554, acc: 0.956, test_loss 0.555, test_acc 1.000\n",
            "CE:0.880  MSE:0.120\n",
            "Epoch 089, loss: 0.559, acc: 0.956, test_loss 0.560, test_acc 1.000\n",
            "CE:0.890  MSE:0.110\n",
            "Epoch 090, loss: 0.564, acc: 0.956, test_loss 0.565, test_acc 1.000\n",
            "CE:0.900  MSE:0.100\n",
            "Epoch 091, loss: 0.569, acc: 0.956, test_loss 0.570, test_acc 1.000\n",
            "CE:0.910  MSE:0.090\n",
            "Epoch 092, loss: 0.574, acc: 0.956, test_loss 0.575, test_acc 1.000\n",
            "CE:0.920  MSE:0.080\n",
            "Epoch 093, loss: 0.579, acc: 0.956, test_loss 0.580, test_acc 1.000\n",
            "CE:0.930  MSE:0.070\n",
            "Epoch 094, loss: 0.585, acc: 0.956, test_loss 0.586, test_acc 1.000\n",
            "CE:0.940  MSE:0.060\n",
            "Epoch 095, loss: 0.590, acc: 0.956, test_loss 0.591, test_acc 1.000\n",
            "CE:0.950  MSE:0.050\n",
            "Epoch 096, loss: 0.595, acc: 0.963, test_loss 0.595, test_acc 1.000\n",
            "CE:0.960  MSE:0.040\n",
            "Epoch 097, loss: 0.600, acc: 0.963, test_loss 0.601, test_acc 1.000\n",
            "CE:0.970  MSE:0.030\n",
            "Epoch 098, loss: 0.605, acc: 0.963, test_loss 0.606, test_acc 1.000\n",
            "CE:0.980  MSE:0.020\n",
            "Epoch 099, loss: 0.611, acc: 0.963, test_loss 0.610, test_acc 1.000\n",
            "CE:0.990  MSE:0.010\n",
            "Epoch 100, loss: 0.616, acc: 0.963, test_loss 0.615, test_acc 1.000\n",
            "CE:1.000  MSE:-0.000\n",
            "Fold 2\n",
            "Epoch 001, loss: 0.239, acc: 0.326, test_loss 0.216, test_acc 0.333\n",
            "CE:0.010  MSE:0.990\n",
            "Epoch 002, loss: 0.226, acc: 0.356, test_loss 0.199, test_acc 0.333\n",
            "CE:0.020  MSE:0.980\n",
            "Epoch 003, loss: 0.209, acc: 0.489, test_loss 0.185, test_acc 0.667\n",
            "CE:0.030  MSE:0.970\n",
            "Epoch 004, loss: 0.197, acc: 0.711, test_loss 0.177, test_acc 0.733\n",
            "CE:0.040  MSE:0.960\n",
            "Epoch 005, loss: 0.191, acc: 0.711, test_loss 0.175, test_acc 0.667\n",
            "CE:0.050  MSE:0.950\n",
            "Epoch 006, loss: 0.190, acc: 0.667, test_loss 0.176, test_acc 0.667\n",
            "CE:0.060  MSE:0.940\n",
            "Epoch 007, loss: 0.191, acc: 0.667, test_loss 0.178, test_acc 0.667\n",
            "CE:0.070  MSE:0.930\n",
            "Epoch 008, loss: 0.194, acc: 0.667, test_loss 0.181, test_acc 0.667\n",
            "CE:0.080  MSE:0.920\n",
            "Epoch 009, loss: 0.197, acc: 0.689, test_loss 0.185, test_acc 0.733\n",
            "CE:0.090  MSE:0.910\n",
            "Epoch 010, loss: 0.200, acc: 0.726, test_loss 0.189, test_acc 0.733\n",
            "CE:0.100  MSE:0.900\n",
            "Epoch 011, loss: 0.204, acc: 0.756, test_loss 0.193, test_acc 0.800\n",
            "CE:0.110  MSE:0.890\n",
            "Epoch 012, loss: 0.207, acc: 0.800, test_loss 0.197, test_acc 0.867\n",
            "CE:0.120  MSE:0.880\n",
            "Epoch 013, loss: 0.211, acc: 0.800, test_loss 0.201, test_acc 0.867\n",
            "CE:0.130  MSE:0.870\n",
            "Epoch 014, loss: 0.215, acc: 0.800, test_loss 0.205, test_acc 0.867\n",
            "CE:0.140  MSE:0.860\n",
            "Epoch 015, loss: 0.218, acc: 0.807, test_loss 0.210, test_acc 0.867\n",
            "CE:0.150  MSE:0.850\n",
            "Epoch 016, loss: 0.222, acc: 0.807, test_loss 0.214, test_acc 0.867\n",
            "CE:0.160  MSE:0.840\n",
            "Epoch 017, loss: 0.226, acc: 0.822, test_loss 0.219, test_acc 0.867\n",
            "CE:0.170  MSE:0.830\n",
            "Epoch 018, loss: 0.230, acc: 0.815, test_loss 0.224, test_acc 0.867\n",
            "CE:0.180  MSE:0.820\n",
            "Epoch 019, loss: 0.235, acc: 0.815, test_loss 0.228, test_acc 0.867\n",
            "CE:0.190  MSE:0.810\n",
            "Epoch 020, loss: 0.239, acc: 0.822, test_loss 0.233, test_acc 0.867\n",
            "CE:0.200  MSE:0.800\n",
            "Epoch 021, loss: 0.243, acc: 0.822, test_loss 0.238, test_acc 0.867\n",
            "CE:0.210  MSE:0.790\n",
            "Epoch 022, loss: 0.248, acc: 0.822, test_loss 0.243, test_acc 0.867\n",
            "CE:0.220  MSE:0.780\n",
            "Epoch 023, loss: 0.252, acc: 0.822, test_loss 0.249, test_acc 0.867\n",
            "CE:0.230  MSE:0.770\n",
            "Epoch 024, loss: 0.257, acc: 0.830, test_loss 0.254, test_acc 0.867\n",
            "CE:0.240  MSE:0.760\n",
            "Epoch 025, loss: 0.262, acc: 0.837, test_loss 0.259, test_acc 0.867\n",
            "CE:0.250  MSE:0.750\n",
            "Epoch 026, loss: 0.267, acc: 0.837, test_loss 0.265, test_acc 0.867\n",
            "CE:0.260  MSE:0.740\n",
            "Epoch 027, loss: 0.272, acc: 0.844, test_loss 0.271, test_acc 0.867\n",
            "CE:0.270  MSE:0.730\n",
            "Epoch 028, loss: 0.277, acc: 0.844, test_loss 0.276, test_acc 0.867\n",
            "CE:0.280  MSE:0.720\n",
            "Epoch 029, loss: 0.282, acc: 0.844, test_loss 0.282, test_acc 0.867\n",
            "CE:0.290  MSE:0.710\n",
            "Epoch 030, loss: 0.287, acc: 0.844, test_loss 0.287, test_acc 0.867\n",
            "CE:0.300  MSE:0.700\n",
            "Epoch 031, loss: 0.292, acc: 0.830, test_loss 0.293, test_acc 0.867\n",
            "CE:0.310  MSE:0.690\n",
            "Epoch 032, loss: 0.298, acc: 0.830, test_loss 0.299, test_acc 0.867\n",
            "CE:0.320  MSE:0.680\n",
            "Epoch 033, loss: 0.303, acc: 0.830, test_loss 0.305, test_acc 0.867\n",
            "CE:0.330  MSE:0.670\n",
            "Epoch 034, loss: 0.309, acc: 0.830, test_loss 0.311, test_acc 0.867\n",
            "CE:0.340  MSE:0.660\n",
            "Epoch 035, loss: 0.314, acc: 0.837, test_loss 0.317, test_acc 0.867\n",
            "CE:0.350  MSE:0.650\n",
            "Epoch 036, loss: 0.320, acc: 0.844, test_loss 0.323, test_acc 0.867\n",
            "CE:0.360  MSE:0.640\n",
            "Epoch 037, loss: 0.325, acc: 0.844, test_loss 0.329, test_acc 0.867\n",
            "CE:0.370  MSE:0.630\n",
            "Epoch 038, loss: 0.331, acc: 0.844, test_loss 0.335, test_acc 0.867\n",
            "CE:0.380  MSE:0.620\n",
            "Epoch 039, loss: 0.336, acc: 0.852, test_loss 0.341, test_acc 0.867\n",
            "CE:0.390  MSE:0.610\n",
            "Epoch 040, loss: 0.342, acc: 0.859, test_loss 0.347, test_acc 0.867\n",
            "CE:0.400  MSE:0.600\n",
            "Epoch 041, loss: 0.348, acc: 0.867, test_loss 0.353, test_acc 0.867\n",
            "CE:0.410  MSE:0.590\n",
            "Epoch 042, loss: 0.353, acc: 0.852, test_loss 0.359, test_acc 0.867\n",
            "CE:0.420  MSE:0.580\n",
            "Epoch 043, loss: 0.359, acc: 0.859, test_loss 0.365, test_acc 0.867\n",
            "CE:0.430  MSE:0.570\n",
            "Epoch 044, loss: 0.364, acc: 0.867, test_loss 0.371, test_acc 0.867\n",
            "CE:0.440  MSE:0.560\n",
            "Epoch 045, loss: 0.370, acc: 0.867, test_loss 0.377, test_acc 0.867\n",
            "CE:0.450  MSE:0.550\n",
            "Epoch 046, loss: 0.376, acc: 0.874, test_loss 0.383, test_acc 0.867\n",
            "CE:0.460  MSE:0.540\n",
            "Epoch 047, loss: 0.381, acc: 0.874, test_loss 0.389, test_acc 0.867\n",
            "CE:0.470  MSE:0.530\n",
            "Epoch 048, loss: 0.387, acc: 0.874, test_loss 0.395, test_acc 0.867\n",
            "CE:0.480  MSE:0.520\n",
            "Epoch 049, loss: 0.392, acc: 0.889, test_loss 0.401, test_acc 0.867\n",
            "CE:0.490  MSE:0.510\n",
            "Epoch 050, loss: 0.398, acc: 0.881, test_loss 0.407, test_acc 0.867\n",
            "CE:0.500  MSE:0.500\n",
            "Epoch 051, loss: 0.404, acc: 0.889, test_loss 0.413, test_acc 0.867\n",
            "CE:0.510  MSE:0.490\n",
            "Epoch 052, loss: 0.409, acc: 0.889, test_loss 0.419, test_acc 0.867\n",
            "CE:0.520  MSE:0.480\n",
            "Epoch 053, loss: 0.415, acc: 0.889, test_loss 0.425, test_acc 0.867\n",
            "CE:0.530  MSE:0.470\n",
            "Epoch 054, loss: 0.420, acc: 0.889, test_loss 0.431, test_acc 0.867\n",
            "CE:0.540  MSE:0.460\n",
            "Epoch 055, loss: 0.426, acc: 0.889, test_loss 0.437, test_acc 0.867\n",
            "CE:0.550  MSE:0.450\n",
            "Epoch 056, loss: 0.431, acc: 0.889, test_loss 0.443, test_acc 0.867\n",
            "CE:0.560  MSE:0.440\n",
            "Epoch 057, loss: 0.437, acc: 0.889, test_loss 0.449, test_acc 0.867\n",
            "CE:0.570  MSE:0.430\n",
            "Epoch 058, loss: 0.443, acc: 0.889, test_loss 0.455, test_acc 0.867\n",
            "CE:0.580  MSE:0.420\n",
            "Epoch 059, loss: 0.448, acc: 0.889, test_loss 0.460, test_acc 0.867\n",
            "CE:0.590  MSE:0.410\n",
            "Epoch 060, loss: 0.454, acc: 0.889, test_loss 0.467, test_acc 0.867\n",
            "CE:0.600  MSE:0.400\n",
            "Epoch 061, loss: 0.459, acc: 0.889, test_loss 0.472, test_acc 0.867\n",
            "CE:0.610  MSE:0.390\n",
            "Epoch 062, loss: 0.465, acc: 0.889, test_loss 0.478, test_acc 0.867\n",
            "CE:0.620  MSE:0.380\n",
            "Epoch 063, loss: 0.470, acc: 0.889, test_loss 0.484, test_acc 0.867\n",
            "CE:0.630  MSE:0.370\n",
            "Epoch 064, loss: 0.476, acc: 0.889, test_loss 0.490, test_acc 0.867\n",
            "CE:0.640  MSE:0.360\n",
            "Epoch 065, loss: 0.481, acc: 0.889, test_loss 0.495, test_acc 0.867\n",
            "CE:0.650  MSE:0.350\n",
            "Epoch 066, loss: 0.486, acc: 0.889, test_loss 0.501, test_acc 0.867\n",
            "CE:0.660  MSE:0.340\n",
            "Epoch 067, loss: 0.492, acc: 0.889, test_loss 0.507, test_acc 0.867\n",
            "CE:0.670  MSE:0.330\n",
            "Epoch 068, loss: 0.497, acc: 0.896, test_loss 0.513, test_acc 0.867\n",
            "CE:0.680  MSE:0.320\n",
            "Epoch 069, loss: 0.503, acc: 0.896, test_loss 0.518, test_acc 0.867\n",
            "CE:0.690  MSE:0.310\n",
            "Epoch 070, loss: 0.508, acc: 0.896, test_loss 0.524, test_acc 0.800\n",
            "CE:0.700  MSE:0.300\n",
            "Epoch 071, loss: 0.513, acc: 0.896, test_loss 0.530, test_acc 0.867\n",
            "CE:0.710  MSE:0.290\n",
            "Epoch 072, loss: 0.519, acc: 0.904, test_loss 0.535, test_acc 0.800\n",
            "CE:0.720  MSE:0.280\n",
            "Epoch 073, loss: 0.524, acc: 0.904, test_loss 0.540, test_acc 0.867\n",
            "CE:0.730  MSE:0.270\n",
            "Epoch 074, loss: 0.530, acc: 0.904, test_loss 0.547, test_acc 0.800\n",
            "CE:0.740  MSE:0.260\n",
            "Epoch 075, loss: 0.535, acc: 0.904, test_loss 0.552, test_acc 0.800\n",
            "CE:0.750  MSE:0.250\n",
            "Epoch 076, loss: 0.540, acc: 0.904, test_loss 0.558, test_acc 0.800\n",
            "CE:0.760  MSE:0.240\n",
            "Epoch 077, loss: 0.545, acc: 0.911, test_loss 0.563, test_acc 0.800\n",
            "CE:0.770  MSE:0.230\n",
            "Epoch 078, loss: 0.551, acc: 0.911, test_loss 0.569, test_acc 0.800\n",
            "CE:0.780  MSE:0.220\n",
            "Epoch 079, loss: 0.556, acc: 0.926, test_loss 0.574, test_acc 0.800\n",
            "CE:0.790  MSE:0.210\n",
            "Epoch 080, loss: 0.561, acc: 0.926, test_loss 0.580, test_acc 0.800\n",
            "CE:0.800  MSE:0.200\n",
            "Epoch 081, loss: 0.567, acc: 0.926, test_loss 0.585, test_acc 0.800\n",
            "CE:0.810  MSE:0.190\n",
            "Epoch 082, loss: 0.572, acc: 0.926, test_loss 0.590, test_acc 0.800\n",
            "CE:0.820  MSE:0.180\n",
            "Epoch 083, loss: 0.577, acc: 0.926, test_loss 0.596, test_acc 0.800\n",
            "CE:0.830  MSE:0.170\n",
            "Epoch 084, loss: 0.582, acc: 0.926, test_loss 0.601, test_acc 0.800\n",
            "CE:0.840  MSE:0.160\n",
            "Epoch 085, loss: 0.587, acc: 0.926, test_loss 0.607, test_acc 0.800\n",
            "CE:0.850  MSE:0.150\n",
            "Epoch 086, loss: 0.592, acc: 0.926, test_loss 0.612, test_acc 0.800\n",
            "CE:0.860  MSE:0.140\n",
            "Epoch 087, loss: 0.597, acc: 0.926, test_loss 0.617, test_acc 0.800\n",
            "CE:0.870  MSE:0.130\n",
            "Epoch 088, loss: 0.603, acc: 0.926, test_loss 0.623, test_acc 0.800\n",
            "CE:0.880  MSE:0.120\n",
            "Epoch 089, loss: 0.608, acc: 0.926, test_loss 0.628, test_acc 0.800\n",
            "CE:0.890  MSE:0.110\n",
            "Epoch 090, loss: 0.613, acc: 0.926, test_loss 0.634, test_acc 0.800\n",
            "CE:0.900  MSE:0.100\n",
            "Epoch 091, loss: 0.618, acc: 0.933, test_loss 0.639, test_acc 0.800\n",
            "CE:0.910  MSE:0.090\n",
            "Epoch 092, loss: 0.623, acc: 0.933, test_loss 0.644, test_acc 0.800\n",
            "CE:0.920  MSE:0.080\n",
            "Epoch 093, loss: 0.628, acc: 0.933, test_loss 0.649, test_acc 0.800\n",
            "CE:0.930  MSE:0.070\n",
            "Epoch 094, loss: 0.633, acc: 0.941, test_loss 0.655, test_acc 0.800\n",
            "CE:0.940  MSE:0.060\n",
            "Epoch 095, loss: 0.638, acc: 0.941, test_loss 0.660, test_acc 0.800\n",
            "CE:0.950  MSE:0.050\n",
            "Epoch 096, loss: 0.643, acc: 0.941, test_loss 0.666, test_acc 0.800\n",
            "CE:0.960  MSE:0.040\n",
            "Epoch 097, loss: 0.648, acc: 0.941, test_loss 0.671, test_acc 0.867\n",
            "CE:0.970  MSE:0.030\n",
            "Epoch 098, loss: 0.652, acc: 0.941, test_loss 0.676, test_acc 0.867\n",
            "CE:0.980  MSE:0.020\n",
            "Epoch 099, loss: 0.658, acc: 0.941, test_loss 0.681, test_acc 0.867\n",
            "CE:0.990  MSE:0.010\n",
            "Epoch 100, loss: 0.662, acc: 0.941, test_loss 0.686, test_acc 0.867\n",
            "CE:1.000  MSE:-0.000\n",
            "Fold 3\n",
            "Epoch 001, loss: 0.239, acc: 0.326, test_loss 0.230, test_acc 0.333\n",
            "CE:0.010  MSE:0.990\n",
            "Epoch 002, loss: 0.230, acc: 0.333, test_loss 0.217, test_acc 0.400\n",
            "CE:0.020  MSE:0.980\n",
            "Epoch 003, loss: 0.218, acc: 0.407, test_loss 0.203, test_acc 0.467\n",
            "CE:0.030  MSE:0.970\n",
            "Epoch 004, loss: 0.208, acc: 0.600, test_loss 0.193, test_acc 0.733\n",
            "CE:0.040  MSE:0.960\n",
            "Epoch 005, loss: 0.200, acc: 0.689, test_loss 0.187, test_acc 0.733\n",
            "CE:0.050  MSE:0.950\n",
            "Epoch 006, loss: 0.198, acc: 0.681, test_loss 0.186, test_acc 0.733\n",
            "CE:0.060  MSE:0.940\n",
            "Epoch 007, loss: 0.198, acc: 0.674, test_loss 0.187, test_acc 0.667\n",
            "CE:0.070  MSE:0.930\n",
            "Epoch 008, loss: 0.201, acc: 0.667, test_loss 0.190, test_acc 0.667\n",
            "CE:0.080  MSE:0.920\n",
            "Epoch 009, loss: 0.204, acc: 0.667, test_loss 0.193, test_acc 0.667\n",
            "CE:0.090  MSE:0.910\n",
            "Epoch 010, loss: 0.208, acc: 0.681, test_loss 0.196, test_acc 0.667\n",
            "CE:0.100  MSE:0.900\n",
            "Epoch 011, loss: 0.212, acc: 0.689, test_loss 0.199, test_acc 0.733\n",
            "CE:0.110  MSE:0.890\n",
            "Epoch 012, loss: 0.216, acc: 0.704, test_loss 0.203, test_acc 0.867\n",
            "CE:0.120  MSE:0.880\n",
            "Epoch 013, loss: 0.221, acc: 0.741, test_loss 0.206, test_acc 0.867\n",
            "CE:0.130  MSE:0.870\n",
            "Epoch 014, loss: 0.225, acc: 0.756, test_loss 0.210, test_acc 0.867\n",
            "CE:0.140  MSE:0.860\n",
            "Epoch 015, loss: 0.229, acc: 0.778, test_loss 0.214, test_acc 0.867\n",
            "CE:0.150  MSE:0.850\n",
            "Epoch 016, loss: 0.234, acc: 0.800, test_loss 0.218, test_acc 0.867\n",
            "CE:0.160  MSE:0.840\n",
            "Epoch 017, loss: 0.238, acc: 0.793, test_loss 0.222, test_acc 0.867\n",
            "CE:0.170  MSE:0.830\n",
            "Epoch 018, loss: 0.243, acc: 0.800, test_loss 0.226, test_acc 0.867\n",
            "CE:0.180  MSE:0.820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-23833902e7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCEscalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMSEscalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7478faa876ed>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(features, labels, model, CEscalar, MSEscalar, optimizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCEscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSEscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Get the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7478faa876ed>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(labels, predictions, CEscalar, MSEscalar)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mCE_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mMSE_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mRUN_FUNCTIONS_EAGERLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7478faa876ed>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(logits, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mce_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mce_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2\u001b[0;34m(labels, logits, axis, name)\u001b[0m\n\u001b[1;32m   3217\u001b[0m   \"\"\"\n\u001b[1;32m   3218\u001b[0m   return softmax_cross_entropy_with_logits_v2_helper(\n\u001b[0;32m-> 3219\u001b[0;31m       labels=labels, logits=logits, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   3220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2_helper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3312\u001b[0m     \u001b[0;31m# Make precise_logits and labels into matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3313\u001b[0;31m     \u001b[0mprecise_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten_outer_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecise_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3314\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten_outer_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_flatten_outer_dims\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m   2921\u001b[0m   \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m   last_dim_size = array_ops.slice(\n\u001b[0;32m-> 2923\u001b[0;31m       array_ops.shape(logits), [math_ops.subtract(rank, 1)], [1])\n\u001b[0m\u001b[1;32m   2924\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_dim_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m   \"\"\"\n\u001b[0;32m--> 602\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   8861\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   8862\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8863\u001b[0;31m         tld.op_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   8864\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8865\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTf8DnTzBI9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(StatsforHybrids)\n",
        "Hybrids = [\"CE1MSE0\",\"CE75MSE25\",\"CE5MSE5\",\"CE25MSE75\",\"CE0MSE1\",\"CEtoMSE\",\"MSEtoCE\"]\n",
        "whitney(StatsforHybrids,str1,0.05,alt=None,savetoFile=True,file=\"IrisStats.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
