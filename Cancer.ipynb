{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cancer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U44wvsRQFg5K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "248ef812-d4c2-457d-b030-e66df23daf56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA_hLy8hUMYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4efb5d77-c901-442b-92e0-ca852590239f"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5O_HFjneDkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn.preprocessing as pp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_openml\n",
        "import sklearn.datasets as sk\n",
        "import csv\n",
        "import math\n",
        "from decimal import Decimal\n",
        "import random\n",
        "from scipy.stats import mannwhitneyu\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYYEqUb4eLKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def cross_entropy(logits, y):\n",
        "    ce_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))    \n",
        "    return ce_op\n",
        "\n",
        "@tf.function\n",
        "def mse(pred, y):\n",
        "    mse_op = tf.reduce_mean(tf.square(pred - y))\n",
        "    return mse_op\n",
        "\n",
        "@tf.function\n",
        "def loss_function(labels,predictions,CEscalar,MSEscalar):\n",
        "\n",
        "  labels = tf.dtypes.cast(labels,tf.float64)\n",
        " \n",
        "  CE_object = cross_entropy(predictions,labels)\n",
        "\n",
        "  MSE_object = mse(predictions,labels)\n",
        " \n",
        " \n",
        "  loss_object = (CEscalar*CE_object)+(MSEscalar*MSE_object)\n",
        "\n",
        "  return loss_object\n",
        "\n",
        "@tf.function\n",
        "def model_test(features, labels,model,CEscalar,MSEscalar):\n",
        "    predictions = model(features,training=False)\n",
        "    t_loss = loss_function(labels, predictions, CEscalar, MSEscalar)\n",
        "    test_loss(t_loss)\n",
        "    test_acc(labels, predictions)\n",
        "    \n",
        "\n",
        "\n",
        "@tf.function\n",
        "def model_train(features, labels,model,CEscalar,MSEscalar,optimizer):\n",
        "    # Define the GradientTape context\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get the probabilities\n",
        "        predictions = model(features, training=True)\n",
        "        # Calculate the loss\n",
        "        loss = loss_function(labels, predictions, CEscalar, MSEscalar)   \n",
        "    # Get the gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "    # Update the loss and accuracy\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, predictions)\n",
        "    return gradients \n",
        "       \n",
        "\n",
        "      \n",
        "def PrintFunc(template):\n",
        "    return template.format(epoch+1,loss, acc,tloss,tacc)\n",
        "\n",
        "def shuffle(features,labels):\n",
        "    #zip data\n",
        "    data = list(zip(features,labels))\n",
        "\n",
        "    #shuffle data\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    #unzip data\n",
        "    features,labels = zip(*data)\n",
        "    features = np.asarray(features)\n",
        "    labels = np.asarray(labels)\n",
        "    return features,labels  \n",
        "\n",
        "\n",
        "\n",
        "def MeanAcrossEpoch(arr,num_epoch):\n",
        "    \n",
        "    tot = []\n",
        "   \n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    for i in arr:\n",
        "        val = 0\n",
        "        val = np.mean(i,dtype=np.float64)\n",
        "        tot.append(val)\n",
        "    return tot      \n",
        "\n",
        "\n",
        "def StdAcrossEpoch(arr,num_epoch):\n",
        "    \n",
        "    tot = []\n",
        "   \n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    for i in arr:\n",
        "        val = 0\n",
        "        val = np.std(i,dtype=np.float64,ddof=1)\n",
        "        tot.append(val)\n",
        "    return tot    \n",
        "\n",
        "\n",
        "def createList(r1, r2): \n",
        "  \n",
        "    # Testing if range r1 and r2  \n",
        "    # are equal \n",
        "    if (r1 == r2): \n",
        "        return r1 \n",
        "  \n",
        "    else: \n",
        "  \n",
        "        # Create empty list \n",
        "        res = [] \n",
        "  \n",
        "        # loop to append successors to  \n",
        "        # list until r2 is reached. \n",
        "        while(r1 < r2+1 ): \n",
        "              \n",
        "            res.append(r1) \n",
        "            r1 += 1\n",
        "        return res \n",
        "          \n",
        "\n",
        "def SameEpoch(arr,num_epoch):\n",
        "    new = []\n",
        "    for i in range(num_epoch):\n",
        "        new.append(arr[i::num_epoch])   \n",
        "    return new\n",
        "\n",
        "#Get the mean for each epoch across the different folds\n",
        "def MeanAcrossFolds(arr,num_epoch,num_folds):\n",
        "    tot = []\n",
        "    arr = SameEpoch(arr,num_epoch)  \n",
        "    \n",
        "    for i in range(num_epoch):\n",
        "        val = 0\n",
        "        val = np.sum(arr[i],dtype=np.float64)/num_folds\n",
        "        tot.append(val)\n",
        "    return tot       \n",
        "\n",
        "\n",
        "\n",
        "#Whitney U test\n",
        "def whitney(data,names,alpha=0.05,alt=None,savetoFile = False,file='default'):\n",
        "\n",
        "  if savetoFile == False:\n",
        "\n",
        "    for x in range(len(data)-1):\n",
        "      print('\\n')\n",
        "      for y in range(x+1,len(data)):\n",
        "\n",
        "        print(names[x] + ' Mean is less than ' + names[y])\n",
        "        stat, p = mannwhitneyu(data[x], data[y] , alternative= alt)\n",
        "        print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "        if p > alpha:\n",
        "\t        print('Same distribution (fail to reject H0)')\n",
        "        else:\n",
        "\t        print('Different distribution (reject H0)')\n",
        "  else:\n",
        "      file = open(\"C:/Users/AutoMAttic/Desktop/honours/COS700/Research/glass\"+file,'a')\n",
        "      for x in range(len(data)-1):\n",
        "        file.write('\\n')\n",
        "        file.write('\\n')\n",
        "        for y in range(x+1,len(data)):\n",
        "          file.write(names[x] + ' Checking distibution ' + names[y])\n",
        "          file.write('\\n')\n",
        "          stat, p = mannwhitneyu(data[x], data[y] , alternative= alt)\n",
        "          file.write('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "          file.write('\\n')\n",
        "          if p > alpha:\n",
        "\t          file.write('Same distribution (fail to reject H0)')\n",
        "          else:\n",
        "\t          file.write('Different distribution (reject H0)')     \n",
        "      file.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsPNxkU7eRaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting Environment\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "\n",
        "\n",
        "# Hyper Parameter\n",
        "LEARNING_RATE = 0.001\n",
        "CE_SCALAR = 1\n",
        "MSE_SCALAR = 0\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_FOLDS = 10\n",
        "RUNS = 30\n",
        "TOTALEPOCHS = NUM_FOLDS*NUM_EPOCHS*RUNS\n",
        "\n",
        "#Metrics\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "train_acc = tf.keras.metrics.BinaryAccuracy(name=\"train_acc\")\n",
        "test_acc = tf.keras.metrics.BinaryAccuracy(name=\"test_acc\")\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Load features and labels from iris\n",
        "features, labels = sk.load_breast_cancer( return_X_y=True)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(features)\n",
        "features = scaler.transform(features)\n",
        "\n",
        "labels = pp.label_binarize(labels, classes=np.unique(labels))\n",
        "\n",
        "# shuffle data\n",
        "features,labels=shuffle(features,labels)\n",
        "\n",
        "#for stats test\n",
        "StatsforHybrids = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-irwq4dCX-V",
        "colab": {}
      },
      "source": [
        "#static hybrid functions\n",
        "while(CE_SCALAR >= 0):\n",
        "\n",
        "    #Setting seed values for consistent results when different hybrids are used\n",
        "    seed = 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "\n",
        "    #Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Declaring arrays for holding mean values across folds\n",
        "    MeantestAccSamples =[]\n",
        "    MeantrainAccSamples = []\n",
        "    MeantestLossSamples =[]\n",
        "    MeantrainLossSamples = []\n",
        "\n",
        "    #Declaring arrays for holding mean values across runs\n",
        "    atr_accuracy_results = []\n",
        "    ate_accuracy_results = []\n",
        "    atr_loss_results = []\n",
        "    ate_loss_results = []\n",
        "\n",
        "    #Defining arrays to be used to flatten values into a one dimensional array \n",
        "    testAccflatten = []\n",
        "    trainAccflatten = []\n",
        "    testLossflatten = []\n",
        "    trainLossflatten = []\n",
        "\n",
        "    #arrays used to capture standard deviation \n",
        "    atrstdacc = []\n",
        "    atestdacc = []\n",
        "    atrstdloss = []\n",
        "    atestdloss = []\n",
        "\n",
        "\n",
        "    for run in tf.range(RUNS):\n",
        "\n",
        "     \n",
        "       \n",
        "\n",
        "       #Constructing the model \n",
        "       model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(30,)),\n",
        "                              tf.keras.layers.Dense(10,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(1,activation='sigmoid'),\n",
        "       ])\n",
        "\n",
        "\n",
        "       #Save weights \n",
        "       Wsave = model.get_weights()\n",
        "       print(\"run {:d}\".format(run+1))\n",
        "\n",
        "       \n",
        "       fold=1 \n",
        "\n",
        "       kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "       # For data in number of folds\n",
        "       for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "           X_train, X_test = features[train], features[test]\n",
        "           y_train, y_test= labels[train], labels[test]    \n",
        "           \n",
        "           #set saved weights\n",
        "           model.set_weights(Wsave) \n",
        "           \n",
        "           print(\"Fold {:d}\".format(fold))\n",
        "           fold = fold + 1 \n",
        "           \n",
        "           #train for a number of epochs\n",
        "           for epoch in tf.range(NUM_EPOCHS):\n",
        "               \n",
        "               #Shuffle dataset for each epoch\n",
        "               tr = list(zip(X_train, y_train))\n",
        "               te = list(zip(X_test, y_test))\n",
        "               np.random.shuffle(tr)\n",
        "               np.random.shuffle(te)\n",
        "               X_train,y_train = zip(*tr) \n",
        "               X_test,y_test = zip(*te) \n",
        "\n",
        "\n",
        "               #Make numpy arrays to feed into model\n",
        "               X_train=np.asarray(X_train, dtype=np.float64)\n",
        "               y_train=np.asarray(y_train, dtype=np.float64)\n",
        "               X_test=np.asarray(X_test, dtype=np.float64)\n",
        "               y_test=np.asarray(y_test, dtype=np.float64)\n",
        "               \n",
        "               \n",
        "               # Train on batch\n",
        "               for i in range(0,len(X_train) // BATCH_SIZE):\n",
        "                   X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                   y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                   gradients=model_train(X, y,model,CE_SCALAR,MSE_SCALAR,optimizer)\n",
        "                   optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "               # Evaluate for each epoch    \n",
        "               model_test(X_test, y_test,model,CE_SCALAR,MSE_SCALAR)   \n",
        "\n",
        "               # Grab the results\n",
        "               (loss, acc) = train_loss.result(), train_acc.result()\n",
        "               (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "               #Appending values for each epoch \n",
        "               testAccSamples.append(tacc)\n",
        "               trainAccSamples.append(acc)\n",
        "               testLossSamples.append(tloss)\n",
        "               trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "               # Clear the current state of the metrics\n",
        "               train_loss.reset_states(), train_acc.reset_states()\n",
        "               test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "               # Local logging\n",
        "               template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "               output = PrintFunc(template)\n",
        "               print(output)   \n",
        "               \n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "       #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "       testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "       trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "       #Appending fold mean values to arrays\n",
        "       MeantestAccSamples.append(testAccSamples)\n",
        "       MeantrainAccSamples.append(trainAccSamples)\n",
        "       MeantestLossSamples.append(testLossSamples)\n",
        "       MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "       #Resetting Arrays to capture values for next run\n",
        "       testAccSamples =[]\n",
        "       trainAccSamples = []\n",
        "       testLossSamples =[]\n",
        "       trainLossSamples = []\n",
        "\n",
        "       #Changing seed values for each run\n",
        "       seed = run + 1\n",
        "       tf.random.set_seed(seed)\n",
        "       np.random.seed(seed)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    #flattening the Mean values from all runs into a one dimensional array\n",
        "    testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "    testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "    trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "    trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "    testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "    testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "    trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "    trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "    #Calculating the standard deviation \n",
        "    atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "    atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "    atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "    atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "    #Calculating the mean across epochs for the multiple runs\n",
        "    atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "    ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "    atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "    ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "    file = open(\"CancerData.csv\",'a')  \n",
        "    file.write(\"CrossEntropy:\"+str(CE_SCALAR) + \" MeanSquared:\"+str(MSE_SCALAR))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n') \n",
        "    file.write(\"Acc Samples:\")\n",
        "    file.write('\\n')\n",
        "    for sample in testAccflatten[epoch]: \n",
        "        file.write(str(sample))\n",
        "        file.write('\\n')\n",
        "    file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n') \n",
        "    file.write(\"Loss Samples:\")\n",
        "    file.write('\\n')\n",
        "    for sample in testLossflatten[epoch]:        \n",
        "        file.write(str(sample))\n",
        "        file.write('\\n')\n",
        "    file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.write('\\n')\n",
        "    file.close()  \n",
        "\n",
        "\n",
        "    StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "    x = createList(0,NUM_EPOCHS-1)\n",
        "    plt.clf()\n",
        "    plt.ylabel(\"Acc\", fontsize=14)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "    plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True,color='k')\n",
        "    #plt.show() \n",
        "    plt.savefig(f\"CE{CE_SCALAR}MSE{MSE_SCALAR}CancerAcc.eps\", bbox_inches='tight')\n",
        "    plt.clf()\n",
        "\n",
        "    plt.ylabel(\"Loss\", fontsize=14)\n",
        "    plt.xlabel(\"Epoch\", fontsize=14)\n",
        "    plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "    plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True,color='k')\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"CE{CE_SCALAR}MSE{MSE_SCALAR}CancerLoss.eps\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "    #Changing the hybrid for next run\n",
        "    CE_SCALAR = CE_SCALAR - 0.25\n",
        "    MSE_SCALAR = MSE_SCALAR + 0.25\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax2LKVoy3qQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adaptive hybrid CE to MSE\n",
        "seed =1\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "CE_SCALAR = 1\n",
        "MSE_SCALAR = 0\n",
        "\n",
        "#Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "testAccSamples =[]\n",
        "trainAccSamples = []\n",
        "testLossSamples =[]\n",
        "trainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across folds\n",
        "MeantestAccSamples =[]\n",
        "MeantrainAccSamples = []\n",
        "MeantestLossSamples =[]\n",
        "MeantrainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across runs\n",
        "atr_accuracy_results = []\n",
        "ate_accuracy_results = []\n",
        "atr_loss_results = []\n",
        "ate_loss_results = []\n",
        "\n",
        "#Defining arrays to be used to flatten values into a one dimensional array \n",
        "testAccflatten = []\n",
        "trainAccflatten = []\n",
        "testLossflatten = []\n",
        "trainLossflatten = []\n",
        "\n",
        "#arrays used to capture standard deviation \n",
        "atrstdacc = []\n",
        "atestdacc = []\n",
        "atrstdloss = []\n",
        "atestdloss = []\n",
        "\n",
        "seed =1\n",
        "\n",
        "for run in range(RUNS):\n",
        "\n",
        "     \n",
        "\n",
        "    CEscalar = CE_SCALAR\n",
        "    MSEscalar = MSE_SCALAR\n",
        "\n",
        "  \n",
        "    #Constructing the model \n",
        "    model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(30,)),\n",
        "                              tf.keras.layers.Dense(10,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(1,activation='sigmoid'),\n",
        "       ])\n",
        "\n",
        "\n",
        "\n",
        "    #Save weights \n",
        "    Wsave = model.get_weights()\n",
        "    print(\"run {:d}\".format(run+1))\n",
        "\n",
        "    #Creating dataset for K-fold\n",
        "    #dataset = ut.make_dataset(features,labels,NUM_FOLDS)\n",
        "    fold=1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "\n",
        "    #For data in number of folds \n",
        "    for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "        X_train, X_test = features[train], features[test]\n",
        "        y_train, y_test= labels[train], labels[test]  \n",
        "\n",
        "        CEscalar = CE_SCALAR\n",
        "        MSEscalar = MSE_SCALAR  \n",
        "\n",
        "        #set saved weights\n",
        "        model.set_weights(Wsave)\n",
        "       \n",
        "        print(\"Fold {:d}\".format(fold))\n",
        "        fold = fold + 1 \n",
        "           \n",
        "        #train for a number of epochs\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "\n",
        "            tr = list(zip(X_train, y_train))\n",
        "            te = list(zip(X_test, y_test))\n",
        "            np.random.shuffle(tr)\n",
        "            np.random.shuffle(te)\n",
        "            X_train,y_train = zip(*tr) \n",
        "            X_test,y_test = zip(*te) \n",
        "\n",
        "            X_train=np.asarray(X_train, dtype=np.float64)\n",
        "            y_train=np.asarray(y_train, dtype=np.float64)\n",
        "            X_test=np.asarray(X_test, dtype=np.float64)\n",
        "            y_test=np.asarray(y_test, dtype=np.float64)\n",
        "\n",
        "\n",
        "            # Train on batch\n",
        "            for i in range(0,len(X_train) // BATCH_SIZE):\n",
        "                X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                gradients=model_train(X, y,model,CEscalar,MSEscalar,optimizer)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "            # Evaluate for each epoch    \n",
        "            model_test(X_test, y_test,model,CEscalar,MSEscalar)   \n",
        "                 \n",
        "\n",
        "            # Grab the results\n",
        "            (loss, acc) = train_loss.result(), train_acc.result()\n",
        "            (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "            #Appending values for each epoch \n",
        "            testAccSamples.append(tacc)\n",
        "            trainAccSamples.append(acc)\n",
        "            testLossSamples.append(tloss)\n",
        "            trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "            #Clear the current state of the metrics\n",
        "            train_loss.reset_states(), train_acc.reset_states()\n",
        "            test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "            #Local logging\n",
        "            template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "            output = PrintFunc(template)\n",
        "            print(output)   \n",
        "            CEscalar = CEscalar-0.01\n",
        "            MSEscalar = MSEscalar + 0.01\n",
        "            \n",
        "            print('CE:%.3f  MSE:%.3f' % (CEscalar, MSEscalar))\n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "    #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "    testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "    #Appending fold mean values to arrays\n",
        "    MeantestAccSamples.append(testAccSamples)\n",
        "    MeantrainAccSamples.append(trainAccSamples)\n",
        "    MeantestLossSamples.append(testLossSamples)\n",
        "    MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "    #Resetting Arrays to capture values for next run\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Changing seed values for each run\n",
        "    seed = run + 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#flattening the Mean values from all runs into a one dimensional array\n",
        "testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the standard deviation \n",
        "atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "#Calculating the mean across epochs for the multiple runs\n",
        "atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/CancerDataAdCEtoMSE.csv\",'a')  \n",
        "file.write(\"CEtoMSE\")\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Acc Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testAccflatten[epoch]: \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Loss Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testLossflatten[epoch]:        \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.close()  \n",
        "\n",
        "\n",
        "StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "x = createList(0,NUM_EPOCHS-1)\n",
        "plt.clf()\n",
        "plt.ylabel(\"Acc\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True,color='k')\n",
        "#plt.show() \n",
        "plt.savefig(f\"/content/AccCancerAdaptiveCEtoMSE.eps\", bbox_inches='tight')\n",
        "plt.clf()\n",
        "\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True,color='k')\n",
        "#plt.show()\n",
        "plt.savefig(f\"/content/LossCancerAdaptiveCEtoMSE.eps\", bbox_inches='tight')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_uiiWK-4M4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adaptive hybrid MSE to CE\n",
        "seed =1\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "CE_SCALAR = 0\n",
        "MSE_SCALAR = 1\n",
        "\n",
        "#Declaring arrays for capturing all values during the program execution (For each fold,run and epoch)\n",
        "testAccSamples =[]\n",
        "trainAccSamples = []\n",
        "testLossSamples =[]\n",
        "trainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across folds\n",
        "MeantestAccSamples =[]\n",
        "MeantrainAccSamples = []\n",
        "MeantestLossSamples =[]\n",
        "MeantrainLossSamples = []\n",
        "\n",
        "#Declaring arrays for holding mean values across runs\n",
        "atr_accuracy_results = []\n",
        "ate_accuracy_results = []\n",
        "atr_loss_results = []\n",
        "ate_loss_results = []\n",
        "\n",
        "#Defining arrays to be used to flatten values into a one dimensional array \n",
        "testAccflatten = []\n",
        "trainAccflatten = []\n",
        "testLossflatten = []\n",
        "trainLossflatten = []\n",
        "\n",
        "#arrays used to capture standard deviation \n",
        "atrstdacc = []\n",
        "atestdacc = []\n",
        "atrstdloss = []\n",
        "atestdloss = []\n",
        "\n",
        "seed =1\n",
        "\n",
        "for run in tf.range(RUNS):\n",
        "\n",
        "     \n",
        "\n",
        "    CEscalar = CE_SCALAR\n",
        "    MSEscalar = MSE_SCALAR\n",
        "\n",
        "  \n",
        "    #Constructing the model \n",
        "    model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Flatten(input_shape=(30,)),\n",
        "                              tf.keras.layers.Dense(10,use_bias=True,bias_initializer='ones', activation='relu'),\n",
        "                              tf.keras.layers.Dense(1,activation='sigmoid'),\n",
        "       ])\n",
        "\n",
        "\n",
        "\n",
        "    #Save weights \n",
        "    Wsave = model.get_weights()\n",
        "    print(\"run {:d}\".format(run+1))\n",
        "\n",
        "    #Creating dataset for K-fold\n",
        "    #dataset = ut.make_dataset(features,labels,NUM_FOLDS)\n",
        "    fold=1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=74)\n",
        "\n",
        "    #For data in number of folds \n",
        "    for i, (train, test) in enumerate(kfold.split(features, labels.argmax(1))):\n",
        "\n",
        "        X_train, X_test = features[train], features[test]\n",
        "        y_train, y_test= labels[train], labels[test]  \n",
        "\n",
        "        CEscalar = CE_SCALAR\n",
        "        MSEscalar = MSE_SCALAR  \n",
        "\n",
        "        #set saved weights\n",
        "        model.set_weights(Wsave)\n",
        "       \n",
        "        print(\"Fold {:d}\".format(fold))\n",
        "        fold = fold + 1 \n",
        "           \n",
        "        #train for a number of epochs\n",
        "        for epoch in tf.range(NUM_EPOCHS):\n",
        "\n",
        "\n",
        "            tr = list(zip(X_train, y_train))\n",
        "            te = list(zip(X_test, y_test))\n",
        "            np.random.shuffle(tr)\n",
        "            np.random.shuffle(te)\n",
        "            X_train,y_train = zip(*tr) \n",
        "            X_test,y_test = zip(*te) \n",
        "\n",
        "            X_train=np.asarray(X_train, dtype=np.float64)\n",
        "            y_train=np.asarray(y_train, dtype=np.float64)\n",
        "            X_test=np.asarray(X_test, dtype=np.float64)\n",
        "            y_test=np.asarray(y_test, dtype=np.float64)\n",
        "\n",
        "\n",
        "            # Train on batch\n",
        "            for i in tf.range(0,len(X_train) // BATCH_SIZE):\n",
        "                X = X_train[i * BATCH_SIZE:min(len(X_train), (i+1) * BATCH_SIZE)]\n",
        "                y = y_train[i * BATCH_SIZE:min(len(y_train), (i+1) * BATCH_SIZE)]\n",
        "                gradients=model_train(X, y,model,CEscalar,MSEscalar,optimizer)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))      \n",
        "\n",
        "            # Evaluate for each epoch    \n",
        "            model_test(X_test, y_test,model,CEscalar,MSEscalar)   \n",
        "                 \n",
        "\n",
        "            # Grab the results\n",
        "            (loss, acc) = train_loss.result(), train_acc.result()\n",
        "            (tloss, tacc) = test_loss.result(), test_acc.result()\n",
        "\n",
        "\n",
        "            #Appending values for each epoch \n",
        "            testAccSamples.append(tacc)\n",
        "            trainAccSamples.append(acc)\n",
        "            testLossSamples.append(tloss)\n",
        "            trainLossSamples.append(loss)\n",
        "\n",
        "\n",
        "            #Clear the current state of the metrics\n",
        "            train_loss.reset_states(), train_acc.reset_states()\n",
        "            test_loss.reset_states(), test_acc.reset_states()\n",
        "\n",
        "            \n",
        "            #Local logging\n",
        "            template = \"Epoch {:03d}, loss: {:.3f}, acc: {:.3f}, test_loss {:.3f}, test_acc {:.3f}\"\n",
        "            output = PrintFunc(template)\n",
        "            print(output)   \n",
        "            CEscalar = CEscalar+0.01\n",
        "            MSEscalar = MSEscalar - 0.01\n",
        "            \n",
        "            print('CE:%.3f  MSE:%.3f' % (CEscalar, MSEscalar))\n",
        "            \n",
        "       \n",
        "\n",
        "\n",
        "    #Using MeanAcrossFolds to get the mean for each epoch across the different folds\n",
        "    testAccSamples = MeanAcrossFolds(testAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainAccSamples = MeanAcrossFolds(trainAccSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    testLossSamples  = MeanAcrossFolds(testLossSamples  ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    trainLossSamples = MeanAcrossFolds(trainLossSamples ,NUM_EPOCHS,NUM_FOLDS)\n",
        "    \n",
        "    #Appending fold mean values to arrays\n",
        "    MeantestAccSamples.append(testAccSamples)\n",
        "    MeantrainAccSamples.append(trainAccSamples)\n",
        "    MeantestLossSamples.append(testLossSamples)\n",
        "    MeantrainLossSamples.append(trainLossSamples)\n",
        "\n",
        "    #Resetting Arrays to capture values for next run\n",
        "    testAccSamples =[]\n",
        "    trainAccSamples = []\n",
        "    testLossSamples =[]\n",
        "    trainLossSamples = []\n",
        "\n",
        "    #Changing seed values for each run\n",
        "    seed = run + 1\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#flattening the Mean values from all runs into a one dimensional array\n",
        "testAccflatten = [item for sublist in MeantestAccSamples for item in sublist]\n",
        "testAccflatten = SameEpoch(testAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainAccflatten = [item for sublist in MeantrainAccSamples for item in sublist]\n",
        "trainAccflatten = SameEpoch(trainAccflatten ,NUM_EPOCHS)\n",
        "\n",
        "testLossflatten = [item for sublist in MeantestLossSamples for item in sublist]\n",
        "testLossflatten = SameEpoch(testLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "trainLossflatten = [item for sublist in MeantrainLossSamples for item in sublist]\n",
        "trainLossflatten = SameEpoch(trainLossflatten ,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "#Calculating the standard deviation \n",
        "atrstdacc = StdAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "atestdacc = StdAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atrstdloss = StdAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "atestdloss = StdAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "    \n",
        "#Calculating the mean across epochs for the multiple runs\n",
        "atr_accuracy_results= MeanAcrossEpoch(trainAccflatten,NUM_EPOCHS)\n",
        "ate_accuracy_results= MeanAcrossEpoch(testAccflatten,NUM_EPOCHS)\n",
        "atr_loss_results= MeanAcrossEpoch(trainLossflatten,NUM_EPOCHS)\n",
        "ate_loss_results= MeanAcrossEpoch(testLossflatten,NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"/content/CancerDataAdMSEtoCE.csv\",'a')  \n",
        "file.write(\"MSEtoCE\")\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Acc Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testAccflatten[epoch]: \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test Acc mean:'+str(np.mean(testAccflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test Acc std:'+str(np.std(testAccflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n') \n",
        "file.write(\"Loss Samples:\")\n",
        "file.write('\\n')\n",
        "for sample in testLossflatten[epoch]:        \n",
        "    file.write(str(sample))\n",
        "    file.write('\\n')\n",
        "file.write('Test loss mean:'+str(np.mean(testLossflatten[epoch],dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('Test loss std:'+str(np.std(testLossflatten[epoch],ddof=1,dtype = np.float64)))\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.write('\\n')\n",
        "file.close()  \n",
        "\n",
        "\n",
        "StatsforHybrids.append(testAccflatten[epoch])\n",
        " \n",
        "x = createList(0,NUM_EPOCHS-1)\n",
        "plt.clf()\n",
        "plt.ylabel(\"Acc\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_accuracy_results,yerr=atestdacc ,color='b', label = 'test',capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_accuracy_results,yerr=atrstdacc ,color='r', label = 'train',capsize=1, capthick=1, errorevery=2)\n",
        "        \n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True,color='k')\n",
        "#plt.show() \n",
        "plt.savefig(f\"/content/AccCancerAdaptiveMSEtoCE.eps\", bbox_inches='tight')\n",
        "plt.clf()\n",
        "\n",
        "plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.errorbar(x=x,y=ate_loss_results,yerr=atestdloss ,color='b', label = 'test', capsize=1, capthick=1, errorevery=2)\n",
        "plt.errorbar(x=x,y=atr_loss_results,yerr=atrstdloss,color='r', label = 'train', capsize=1, capthick=1, errorevery=2)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True,color='k')\n",
        "#plt.show()\n",
        "plt.savefig(f\"/content/LossCancerAdaptiveMSEtoCE.eps\", bbox_inches='tight')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0UUjeCBBabZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(StatsforHybrids)\n",
        "Hybrids = [\"CE1MSE0\",\"CE75MSE25\",\"CE5MSE5\",\"CE25MSE75\",\"CE0MSE1\",\"CEtoMSE\",\"MSEtoCE\"]\n",
        "whitney(StatsforHybrids,str1,0.05,alt=None,savetoFile=True,file=\"CancerStats.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
